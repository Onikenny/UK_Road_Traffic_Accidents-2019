{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3108cd49",
   "metadata": {},
   "source": [
    "# Road Traffic Accidents Analysis UK (2019)\n",
    "\n",
    "Investigative analysis and predictive modelling based on road traffic accidents reported in the United Kingdom in 2019.\n",
    "\n",
    "The data consists of three individual but related datasets\n",
    "\n",
    "- **Accident dataset**  Accidents: 32 variables, detailing location, time, date, lighting, weather, road conditions and other variables. The unique accident index identifies each observation and make up one of 117,536 collisions\n",
    "- **Vehicle dataset** Contains details of vehicles involved in the accidents.\n",
    "- **Casualties dataset** 16 columns with information about casualties involved in accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d27450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import numpy as np #for linear algebra/data preprocessing\n",
    "import pandas as pd #for data preprocessing\n",
    "import matplotlib.pyplot as plt #For visualization\n",
    "plt.style.use(\"fivethirtyeight\")#For styling the plots\n",
    "import seaborn as sns #for visualization\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #For clustering\n",
    "from scipy.stats import mannwhitneyu, shapiro, ttest_ind #For statistical tests\n",
    "from statsmodels.stats import weightstats as stests #For statistical tests\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler #For Scaling\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder #For Encoding\n",
    "from scipy.spatial.distance import euclidean, cityblock #For distance measurement\n",
    "import category_encoders as ce #For Encoding\n",
    "from sklearn.decomposition import PCA #For Principal Components Analysis\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression #To check for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba575a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedStratifiedKFold, RepeatedKFold #To split data\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,GradientBoostingClassifier,GradientBoostingRegressor# Algorithms for predictions\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor # predicting algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression # predicting algorithms\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, f1_score, classification_report, confusion_matrix #For evaluating built models\n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27935cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "Accidents = pd.read_csv('Road Safety Data - Accidents 2019.csv', parse_dates = True)# Read in the Accidents dataset\n",
    "casualties = pd.read_csv('Road Safety Data - Casualties 2019.csv', parse_dates = True)# Read in the casualties dataset\n",
    "vehicles = pd.read_csv('Road Safety Data- Vehicles 2019.csv', parse_dates = True)# Read in the vehicles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48ed3b",
   "metadata": {},
   "source": [
    "## Initial EDA\n",
    "Having imported appropiate libraries such as Pandas, Numpy, Matplotlib, Seaborn and Scipy, The dataset is imported to and a profile report is generated to identify inconsistences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63386a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_profiling import ProfileReport\n",
    "#profile = ProfileReport(accidents, title=\"Accidents Profile Report\")\n",
    "#profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cf650",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "***\n",
    "Having imported the appropiate libraries such as Pandas, Numpy, Matplotlib, Seaborn and Scipy, I imported the dataset to get an idea of what I'm working with\n",
    "Associated Steps:\n",
    "\n",
    "- Examine data information\n",
    "- Inspect data for missing values.\n",
    "- Handling Missing values.\n",
    "- Merge datasets, re-examine data information and handle outstanding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9000cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Accidents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ad4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8518de",
   "metadata": {},
   "outputs": [],
   "source": [
    "casualties.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500c458",
   "metadata": {},
   "source": [
    "From the datasets we can identify one common feature; Accident_Index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_null(data):\n",
    "    '''\n",
    "    This function calculates the number of missing values in columns of a dataset. \n",
    "    It takes a dataframe as its only parameter. \n",
    "    It is implemented using the pandas object method, isnull(). The method method returns a DataFrame object\n",
    "    where all the values are replaced with a Boolean value True for NULL values, and otherwise False.\n",
    "    The rest of the function sums up the count of null values and appends them to variable (miss) which the function returns'''\n",
    "    \n",
    "    miss = data.isnull().sum() # Compute the percentage and assign it to a variable\n",
    "    return miss # Return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d3efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check volume of missing values per column in accidents.\n",
    "Check_null(Accidents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e97cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check volume missing values per column in Vehicles.\n",
    "Check_null(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701d4c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check volume missing values per column in casualties.\n",
    "Check_null(casualties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c709f2",
   "metadata": {},
   "source": [
    "The Accident dataset is the only dataset with missing values in various columns. The columns with missing values are location and time features. \n",
    "\n",
    "#### Handling\n",
    "As the percentages of missing values of said features is less than one percent and the information cannot be inferred from the rest of the dataset, applying forward fill or backward fill will not negatively affect or skew the data. The methods will replace the missing cells with duplicates from the existing data.\n",
    "\n",
    "Forward fill and Backward fill fetch the value from the index before or after the missing value index respectively to fill.\n",
    "\n",
    "In this case, the scale of the change is small, the methods therefore ensure the shape of the dataframe will remain unaffected. The methods are also ideal for temporal features like time\n",
    "\n",
    "Since both affected features need not be normally distributed and the shape of distribution will be retained, forward and backward filling is adequate for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(data, use_method='ffill', way='dataframe', column=None):\n",
    "    '''\n",
    "        Function fills missing values in a dataset or individual column, witha scalar value using multiple methods. \n",
    "        It accepts the following parameters; \n",
    "        a dataframe, \n",
    "        a method to fill the missing values- set to ffill by default, \n",
    "        way - data structure - dataframe or dataframe column.\n",
    "        column- column name - set to none by default.\n",
    "\n",
    "        It applies pandas fillna methods; ffill = forward fill,  backfill == backward fill \n",
    "        and other statistically significant values including;\n",
    "        The median score\n",
    "        The mean score\n",
    "        The modal value \n",
    "    '''\n",
    "    if way == 'dataframe': # Condition to check way as dataframe\n",
    "        if use_method == 'ffill': # Condition to check for use method as forward fill\n",
    "            data.fillna(method='ffill', inplace = True) # forward fill the missing value\n",
    "            return data.isnull().sum() # Return the sum of missing values\n",
    "        elif use_method == 'backfill': # Condition to check for use method as backward fill\n",
    "            data.fillna(method='backfill', inplace = True) # backward fill the missing value\n",
    "            return data.isnull().sum() # Return the sum of missing values\n",
    "        else :\n",
    "            data.fillna(use_method, inplace = True) # use impute to fill the missing value\n",
    "            return data.isnull().sum() # Return the sum of missing values\n",
    "    elif way == 'feature': # Condition to check way as feature\n",
    "        if use_method == 'ffill': # Condition to check for use method as forward fill\n",
    "            data[column].fillna(method='ffill', inplace = True) # forward fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        elif use_method == 'backfill': # Condition to check for use method as backward fill\n",
    "            data[column].fillna(method='backfill', inplace = True) # backward fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        elif use_method == 'median': # Condition to check for use method as median\n",
    "            data[column].fillna(data[column].median(), inplace = True) # Use median to fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        elif use_method == 'mean': # Condition to check for use method as mean\n",
    "            data[column].fillna(data[column].mean(), inplace = True) # Use mean to fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        elif use_method == 'mode': # Condition to check for use method as mode\n",
    "            data[column].fillna(data[column].mode()[0], inplace = True) # Use mode to fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        else :\n",
    "            data[column].fillna(use_method, inplace = True) # use impute to fill the missing value\n",
    "            return data[column].isnull().sum() # Return the sum of missing values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57de6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filling Missing Values\n",
    "fill_missing(Accidents)\n",
    "fill_missing(Accidents, 'backfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d57a0",
   "metadata": {},
   "source": [
    "### Merging the datasets.\n",
    "\n",
    "In order to perform the required analysis for relevant insights to some questions in this project, there is a need to compare common features within the different data frames. The datasets are therefore merged to achieve to facilitate this comparison.\n",
    "\n",
    "applying the Pandas .merge() method requires a common index. The **Accident Index** column is sufficient for our three datasets. Resulting missing values from the merge will be handled with the previously defined method to avoid loss of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95709648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Unique values for the common feature\n",
    "print(Accidents['Accident_Index'].nunique())\n",
    "print(vehicles['Accident_Index'].nunique())\n",
    "print(casualties['Accident_Index'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c80fe",
   "metadata": {},
   "source": [
    "The result of the above cell gives the inference that the three datasets have very similar classes of their common feature, which further justifies joining on that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df354541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging datasets\n",
    "casualty_vehicle = pd.merge(vehicles, casualties, on = \"Accident_Index\", how = \"outer\") #To merge Casualties and Vehicles datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(Accidents,casualty_vehicle, on = \"Accident_Index\", how = \"left\") #To merge casualty_vehicles and accidents together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156513e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated rows\n",
    "df.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb7a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c257004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Accident_Index'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check_null(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39174652",
   "metadata": {},
   "source": [
    "In handling the missing values, we apply forward fill and backward fill since the datasets a common feature; **Accident_Index**. This means that their index will be determined by Accident_Index and all features will match across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb89b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_missing(df) #For forward fill\n",
    "fill_missing(df, 'backfill') #For backward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check_null(df) #Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d7295",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "***\n",
    "As the data is now relatively clean to a reasonable extent, we explore the data for useful insights to our project problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94284965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy to manipulate for analysis\n",
    "Analysis = df.copy() #Create a copy of the merged dataset to use for some analysis\n",
    "Analysis.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469337ad",
   "metadata": {},
   "source": [
    "To effectively analyze the time related features, the values of Time column are converted to datetime objects with the Datetime Python module.\n",
    "\n",
    "NB. Accidents only related analysis wil be carried out on accidents dataset only to ensure unique accidents cases are examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datetime objects\n",
    "\n",
    "# parse the date values to real date objects\n",
    "Accidents['Date'] = pd.to_datetime(Accidents['Date'])\n",
    "\n",
    "Accidents['Real_time'] = pd.DatetimeIndex(Accidents['Time']) #Convert to datetime object\n",
    "\n",
    "# convert to decimal time\n",
    "def convert_time (time):\n",
    "    dec_time = time.hour + time.minute/60\n",
    "    return dec_time\n",
    "\n",
    "Accidents['decimal_time'] = Accidents['Real_time'].apply(lambda x: convert_time(x))\n",
    "\n",
    "# month\n",
    "Accidents['Month'] = Accidents['Date'].dt.month\n",
    "\n",
    "# week\n",
    "Accidents['week'] = Accidents['Date'].dt.week\n",
    "\n",
    "#convert to exact time of week.\n",
    "Accidents['decimal_week'] = Accidents['Date'].dt.week + Accidents['Date'].dt.day/7\n",
    " \n",
    "# hour\n",
    "Accidents['Hour'] = Accidents['Real_time'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview engineered date features\n",
    "Accidents[['Date', 'Month', 'week', 'Day_of_Week', 'Hour', 'decimal_time']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696aaad2",
   "metadata": {},
   "source": [
    "## A. Significant periods of all road traffic accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b65b8d",
   "metadata": {},
   "source": [
    "According to [UK driving skills][]  occur more often during rush hour than other times of day and in a news report by [BBC][] rush hour was defined to be between 06:00 and 09:00 and 16:00 to 19:00 on weekdays.\n",
    "\n",
    "### Are there significant hours of the day, and days of the week, on which accidents occur?\n",
    "\n",
    ">Since our entire data represents the population of the UK and not just a sample, we can make our statiscal inference by directly reviewing the rate of accidents within our periods of interest [Statistical Inference][] We also examine the descriptive statistics of accidents relative to the period\n",
    "\n",
    "The Hour feature has 24 unique values equivalent to the 24 hours in a day.\n",
    "\n",
    "The Day_of_Week feature has 7 unique values equivalent to the 7 days in a week.\n",
    "\n",
    "[UK driving skills]: https://www.ukdrivingskills.co.uk/avoiding-accidents-in-rush-hour/\n",
    "[BBC]: https://www.bbc.co.uk/news/uk-england-42917201\n",
    "[Statistical Inference]: http://vargas-solar.com/data-centric-smart-everything/statistical-inference/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(param1, param2):\n",
    "    '''This function calculates the  rate of one parameter per another'''\n",
    "    rate = param1/param2\n",
    "    return rate\n",
    "\n",
    "def percentage_rate(param1, param2):\n",
    "    '''This function calculates the percentage rate of any event'''\n",
    "    percentage_rate = (param1/param2)*100\n",
    "    return percentage_rate\n",
    "\n",
    "def percent_diff(param1, param2):\n",
    "    '''This function calculates the percentage difference in rates of any event'''\n",
    "    percent_diff = ((param1-param2)/param2)*100\n",
    "    return percent_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfad59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rates for all accidents to compare analysis results with\n",
    "days= 365\n",
    "hours= 8760\n",
    "weeks = 52\n",
    "sum_accidents = len(Accidents)\n",
    "daily_rate = rate(sum_accidents, days)\n",
    "hourly_rate = rate(sum_accidents, hours)\n",
    "weekly_rate = rate(sum_accidents, weeks)\n",
    "print(f'The daily rate of accidents according to our data is {round(daily_rate)} per day')\n",
    "print(f'The hourly rate of accidents according to our data is {round(hourly_rate)} per hour')\n",
    "print(f'The weekly rate of accidents according to our data is {round(weekly_rate)} per week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_stats(data, columns):\n",
    "    '''\n",
    "    This function returns relevant desriptive statistics of a dataframe column,\n",
    "    Frequency statistics best describe categorical data.\n",
    "    The function accepts the dataframe and relevant columns as parameters, \n",
    "    Pandas dataframe methods are applied to get the value_counts and column descriptions. \n",
    "    It prints out various value counts and returns the summary statistics.\n",
    "    '''\n",
    "    cols = data[columns] \n",
    "    for i in cols.columns : # Loop through  columns\n",
    "        \n",
    "        summary_stats = cols.describe()\n",
    "        mode_ = cols[i].mode()[0] # calculate the modal value of the features\n",
    "        percent_mode = percentage_rate(cols[i].value_counts()[mode_], len(cols[i])) # calculate the modal percentage\n",
    "\n",
    "        print(f'modal value for the {i} column is {mode_}') # Print the modal score of the feature\n",
    "        print(f' The frequency of ({mode_}) is : ({cols[i].value_counts()[mode_]})') # To get the frequency of the modal class\n",
    "        print(f'{mode_} makes up {percent_mode}% of the total of the supplied data ') # Print the modal percentage\n",
    "        print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457c59a",
   "metadata": {},
   "source": [
    "## Hours of  day "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075e81e",
   "metadata": {},
   "source": [
    " An inspection of the visualization of Accidents frequency per hour of day may confirm our introspection for the time periods of interest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e288a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plotting frequency graph of the feature of interest'''\n",
    "\n",
    "def plot_freq(data, x):\n",
    "    #define figure size\n",
    "    plt.figure(figsize=(8,6))\n",
    "    ax = sns.countplot(data= data, x=x, color = 'teal')\n",
    "    if x=='Hour':\n",
    "        ax.set_xlabel(\"Hour of Day\")\n",
    "        ax.set_xticklabels([str(i) for i in range(1,25)], size=14, rotation= 90)\n",
    "        \n",
    "    elif x=='Day_of_Week':\n",
    "        ax.set_xlabel(\"Day_of_Week\")\n",
    "        ax.set_xticklabels([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], size=14, rotation=-45)\n",
    "    \n",
    "    elif x=='week':\n",
    "        ax.set_xlabel(\"week\")        \n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize dristribution of road accidents per hour of day\n",
    "all_hrs =plot_freq(Accidents, 'Hour')\n",
    "plt.title('Frequency of Accidents per Hour of day(2019)')\n",
    "plt.show()\n",
    "\n",
    "#list of all figures\n",
    "figs = []\n",
    "figs.append(all_hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e55413",
   "metadata": {},
   "source": [
    "The plot above reveals highly significant occurences of accidents within the day especially between 7am and 8pm, with major spikes in occurences between 3pm and 6pm, 7am and 8am\n",
    "The statistical summary of the hours of accident occurence may reveal more about our question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9342f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Hour']) # Get Summary Statistics of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dffc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of accidents in the modal hour with hourly rate for all accidents\n",
    "mode_count = Accidents['Hour'].value_counts()[17] # all accidents in the 18th hr\n",
    "\n",
    "i = rate(mode_count, days) # get rate for 18th in the whole year\n",
    "\n",
    "i_percent = percent_diff(i, hourly_rate) #compute percentage difference from hourly rate of all accidents\n",
    "\n",
    "print(i, i_percent)\n",
    "\n",
    "def avg_percent(col, mode, param1, param2):\n",
    "    '''calculate rate and percentage change in rates'''\n",
    "    \n",
    "    mode_count = col.value_counts()[mode] # accidents for the specified mode\n",
    "\n",
    "    i = rate(mode_count, param1) # get rate for mode against certain parameter(days or hours)\n",
    "\n",
    "    i_percent = percent_diff(i, param2) #compute percentage difference between rate of mode and an existing rate\n",
    "\n",
    "    print(f'average: {i}, percentage:{i_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of accidents in the modal hour with hourly rate for all \n",
    "avg_percent(Accidents['Hour'], 17, days, hourly_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef84b1b",
   "metadata": {},
   "source": [
    "From Descriptive statistics and simple analysis we infer that,\n",
    "\n",
    "Accidents happen the least at Night specifically between (20:00 and 06:00) with more occurrences during the day time.\n",
    "\n",
    "Accidents that occur within the 18th hour of the day(17:00 - 18:00) are greater than the hourly average (13/hr) by 108% (27.95/hr)\n",
    "\n",
    "Accidents are most frequent at peculiar times in the morning and evening which may be explained by our initial hypothesis. The hours of highest occurs sit well between our proposed \"rush hour\" where traffic is busiest as a result of socio-economic activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a90914",
   "metadata": {},
   "source": [
    "## Days of week \n",
    "\n",
    "As socio-economic activities happen during the business week(mon-friday), we can naturally assume that there's less traffic during weekends and inherently less occurences of accidents. \n",
    "\n",
    "We review the frequency statistics of accidents across all days to appropriately infer for Accidents in 2019. The related feature has seven unique values in the range 1 to 7 equivalent to the respective days in a week. i.e Sunday to Saturday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For days of the week to get days traffic accidents are most likely.\n",
    "all_days = plot_freq(Accidents,'Day_of_Week')\n",
    "plt.title('Frequency of Accidents per Day of the week (2019)')\n",
    "plt.show()\n",
    "figs.append(all_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acaf7c",
   "metadata": {},
   "source": [
    "A quick look at the plot reveals a confirmation of our initial hypothesis. More occurences are recorded during the business week with Friday being the most common day for accidents to happen and a subsequent but not a dramatic decrease in the number of occurences on the weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fractions import Fraction\n",
    "#compute ratio of all occurences on a random weekday to a random weekend day\n",
    "#saturday = Analysis['Day_of_Week'].value_counts()[7]\n",
    "#tuesday = Analysis['Day_of_Week'].value_counts()[2]\n",
    "#ratio = saturday/tuesday\n",
    "#print(f'The ratio of accidents on a random weekday to a random day of the weekend is {Fraction(ratio).limit_denominator()}')\n",
    "      \n",
    "#calculate and compare rate of accidents on the modal day with daily rate for all accidents\n",
    "avg_percent(Accidents['Day_of_Week'], 6, weeks, daily_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76435ae",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The ascending trend of occurences across the week reveals that more accidents happen as the business week progresses. starting on Monday (day 2), through to, and peaks on Friday (day 6).\n",
    "\n",
    "The modal day of road accidents is Friday, equivalent to the class value 6 of the feature.\n",
    "The average of accidents that occur on Friday is greater than the daily average (322/day) by 13.8% (366/friday)\n",
    "\n",
    "We may attribute this to busier commute, a mixture of people commuting from work or school and people going out or traveling for the weekend.\n",
    "\n",
    "The days of the weekend, Sunday and Monday (day 7 and 1) witness relatively less occurrences which may be attributed to less traffic as a result of fewer activities that increase the use of major roads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefaac4",
   "metadata": {},
   "source": [
    "## B. Significant periods of Motorbike accidents\n",
    "\n",
    ">following similar approach (frequentist statistics) to the question of all road accidents, we investigate the significant periods of motorbike accidents.\n",
    "\n",
    "### For motorbikes, are there significant hours of the day, and days of the week, on which accidents occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(data, feature, classes) :\n",
    "    \"\"\"\n",
    "Function to map values of a dataframe column to their code.\n",
    "It accepts a dataframe, column name and the defined class of changes \n",
    "    \"\"\"\n",
    "    for k,v in classes.items() : # loop through the dictionary\n",
    "        data[feature].replace(k,v,inplace=True) # replacement\n",
    "    return data[feature] # Return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb165c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''On information provided in the variable look up, we map appropriate vehicle types to a motorcycle class to aid analysis'''\n",
    "\n",
    "# mapping appropriate values from the Vehicle_Type feature to Motorcycle\n",
    "classes = {2:'Motorcycle (50cc & less)',3:'Motorcycle (51-125cc)',4:'Motorcycle(126-500cc)',\n",
    "          5:'Motorcycle(501cc+)',22:'Mobility scooter',23:'Electric Motorcycle',\n",
    "          97:'unknown Motorcycle'}\n",
    "\n",
    "#Convert appropriate vehicle types to motorcycle\n",
    "Analysis['Vehicle_Type'] = Convert(Analysis,'Vehicle_Type',classes) # call designated function\n",
    "\n",
    "Analysis.Vehicle_Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We create a dataframe for the Motorcycle class \n",
    "by filtering through values in Vehicle_Type feature equal to Motorcycle in the Analysis dataframe'''\n",
    "m_values =[v for v in classes.values()]\n",
    "#get index of accidents involving motorcycles\n",
    "bike_indx = Analysis['Accident_Index'][Analysis['Vehicle_Type'].isin(m_values)]\n",
    "\n",
    "# subset according to derived index to get dataframe of bike accidents\n",
    "Motorcycle = Accidents.loc[Accidents['Accident_Index'].isin(bike_indx)]\n",
    "len(Motorcycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage rate of accidents that involve motorcycles\n",
    "\n",
    "bike_rates = percentage_rate(len(Motorcycle), len(Accidents))\n",
    "\n",
    "print('Accidents involving motorcycles make up {}% of road accidents in the UK in 2019'.format(int(bike_rates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d118a",
   "metadata": {},
   "source": [
    "## Hours of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hr =plot_freq(Motorcycle,'Hour')\n",
    "plt.title('Frequency of Motorcycle accidents per hour')\n",
    "plt.show()\n",
    "figs.append(m_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Motorcycle, ['Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c32d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rates for all bike accidents to compare analysis results with\n",
    "days= 365\n",
    "hours= 8760\n",
    "weeks = 52\n",
    "sum_bike_accidents = len(Motorcycle)\n",
    "daily_bike_rate = rate(sum_bike_accidents, days)\n",
    "hourly_bike_rate = rate(sum_bike_accidents, hours)\n",
    "weekly_bike_rate = rate(sum_bike_accidents, weeks)\n",
    "print(f'The daily rate of bike accidents according to our data is {round(daily_bike_rate)} per day')\n",
    "print(f'The hourly rate of bike accidents according to our data is {round(hourly_bike_rate)} per hour')\n",
    "print(f'The weekly rate of bike accidents according to our data is {round(weekly_bike_rate)} per week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of bike accidents in the modal hour with hourly rate for all bike accidents\n",
    "avg_percent(Motorcycle['Hour'], 17, days, hourly_bike_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b689a",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "From the descriptive statistics and, simple analysis,\n",
    "\n",
    "Motorcycle accidents make up 12% of all accidents of all accidents in the UK in 2019\n",
    "\n",
    "With the 18th hour at the peak, the modal hour has a frequency of 1357 which constitutes 9.60% weekly motorcycle accidents in 2019. \n",
    "\n",
    "motorcycle ccidents that occur within the 18th hour of the day(17:00 - 18:00) are greater than the hourly average (2/hr) by 130.5% (3.7/hr)\n",
    "\n",
    "From the plot, the occurences appear to be seasonal with spikes at certain hours in the day and very drastic reductions at other times. \n",
    "\n",
    "There's a sudden increase in occurences between 8am and 10am followed by a sharp decrease within the next hour. From then on, there's a observed gradual but steady increase through the rest of the day up until the end of rush hour when occurences begin to gradually decline up to less than 250 occurences during the night and early mornings.\n",
    "\n",
    "\n",
    "Most motorcycle accidents happen during the daytime (between 6am and 7pm) with the highest occurences at consistent times with all road accidents. \n",
    "\n",
    "We may accrue the observations to our 'rush hour' theory and a concentration of motorcycle use for courier services within cities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaad096",
   "metadata": {},
   "source": [
    "## Days of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb96516",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_days =plot_freq(Motorcycle,'Day_of_Week')\n",
    "plt.title('Frequency of Motorcycle accidents per Day of the week')\n",
    "plt.show()\n",
    "figs.append(m_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abe179",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Motorcycle, ['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d35669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of bike accidents on the modal day with daily rate for all bike accidents\n",
    "avg_percent(Motorcycle['Day_of_Week'], 6, weeks, daily_bike_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2285f",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The ascending trend of occurences across the week reveal that more accidents happen as the business week progresses. starting with Monday (day 2), through to, and peaking on Friday (day 6).\n",
    "\n",
    "The modal day of road accidents is Friday, equivalent to the class value 6 of the feature. Occurences on Fridays with a frequency of 2332 accidents constituted 16.5% of weekly motorcycle accidents in the UK in 2019. We may attribute this to busier traffic, as a product of the rush hour of the weekend start.\n",
    "\n",
    "\n",
    "The average of bike accidents that occur on Friday is greater than the daily average (39/day) by 15.8% (44.8/friday)\n",
    "\n",
    "The rest of the distribution is however almost uniform across the week as all days saw bewtween 2000 and 2500 motorcycle accidents throughout the year. We can conclude that the occurence of these accidents are not dependent on the day of the week and there is no obvious correlation between the days and the accidents. \n",
    "\n",
    "Based on our Courier services theory, we consider that courier services like food delivery that make use of motorcycles, carry on regardless of workdays as such services are relatively always in demand. Besides courier services, the largest owners and drivers of motorcycles are young men between the ages 17 and 29 ([National Travel Survey][]).  \n",
    "\n",
    "These statistical observations are similar to the distribution of all road accident occurences, with the major difference being a consistently high frequency of motorcycle accidents across all days in the week.\n",
    "    \n",
    "\n",
    "[National Travel Survey]: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/694965/motorcycle-use-in-england.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5da64b",
   "metadata": {},
   "source": [
    "# C. Significant periods of accidents involving pedestrians\n",
    "\n",
    "### For pedestrians, are there significant hours of the day, and days of the week, on which they are more likely to be involved in an accident?\n",
    "\n",
    "We may start by investigating the assumption that pedestrians are at a higher risk of traffic accidents at night when compared to daytime because of reduced visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587852ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''On information provided in the variable look up, \n",
    "we map appropriate casualty values to a casualty class to aid analysis'''\n",
    "\n",
    "#Analysis['Casualty_Class'].replace(3,'Pedestrian',inplace=True)\n",
    "#Analysis['Casualty_Class'].replace(2,'Passenger',inplace=True)\n",
    "#Analysis['Casualty_Class'].replace(1,'Driver or Rider',inplace=True)\n",
    "\n",
    "#map Casualty_Class value to equivalent catgorical feature\n",
    "Casualty = {1:'Driver or rider',2:'Passenger',3:'Pedestrian'}\n",
    "\n",
    "#Convert appropriate casualty value to casualty category\n",
    "Analysis['Casualty_Class'] = Convert(Analysis,'Casualty_Class', Casualty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We create a dataframe for the Pedestrian class \n",
    "by filtering through values in the Casualty_Class feature equal to Pedestrian in the Analysis dataframe'''\n",
    "\n",
    "#get index of accidents involving pedestrians\n",
    "ped_indx = Analysis['Accident_Index'][Analysis['Casualty_Class']== 'Pedestrian']\n",
    "\n",
    "# subset according to derived index to get dataframe of bike accidents\n",
    "Pedestrian = Accidents.loc[Accidents['Accident_Index'].isin(ped_indx)]\n",
    "len(Pedestrian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rates for all accidents inolving pedestrians to compare analysis results with\n",
    "days= 365\n",
    "hours= 8760\n",
    "weeks = 52\n",
    "sum_ped_accidents = len(Pedestrian)\n",
    "daily_ped_rate = rate(sum_ped_accidents, days)\n",
    "hourly_ped_rate = rate(sum_ped_accidents, hours)\n",
    "weekly_ped_rate = rate(sum_ped_accidents, weeks)\n",
    "print(f'The daily rate of accidents involving pedestrians according to our data is {round(daily_ped_rate)} per day')\n",
    "print(f'The hourly rate of accidents involving pedestrians according to our data is {round(hourly_ped_rate)} per hour')\n",
    "print(f'The weekly rate of accidents involving pedestrians according to our data is {round(weekly_ped_rate)} per week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75090bc7",
   "metadata": {},
   "source": [
    "## Hours of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hr =plot_freq(Pedestrian, 'Hour' )\n",
    "plt.title('Frequency of Pedestrian accidents per hour')\n",
    "plt.show()\n",
    "figs.append(p_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Pedestrian, ['Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of pedestrian accidents in the modal hour with hourly rate for all pedestrian accidents\n",
    "avg_percent(Pedestrian['Hour'], 15, days, hourly_ped_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1856f6",
   "metadata": {},
   "source": [
    "From descriptive statistics and, basic analysis,\n",
    "\n",
    "There are relatively higher occurences of accidents involving pedestrians occuring during the day (between 7am and 7pm) than at other times. \n",
    "\n",
    "The distribution shows a surge at 08:00 hours with respect to other periods of the morning. The most significant hours are between 15:00 hours and 18:00 hours. However, unlike with vehicular accidents whose peak values are within the 17th hour, \n",
    "the highest occurences of pedestrian accidents happen within the 15th hour of the day. pedestrians are involved in accidents at a rate of 6.5/hr which is 145% higher than the hourly average 3/hr.\n",
    "\n",
    "The numbers go up to 2364 occurences which constitutes 10.21% of the total pedestrian accidents that happen daily. \n",
    "\n",
    "The highest periods of pedestrian traffic correlates with Vehicular traffic as well. We can infer that the same theories apply to both categories of accidents.\n",
    "\n",
    "\n",
    "Socio-economic activities have a high influence on the concentration of traffic at various times of the day and heavier traffic inversely implies a higher risk of accidents including those involving pedestrians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139b96a",
   "metadata": {},
   "source": [
    "## Days of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915af411",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_days = plot_freq(Pedestrian, 'Day_of_Week')\n",
    "plt.title('Frequency of Pedestrian accidents per Day of the week')\n",
    "plt.show()\n",
    "figs.append(p_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Pedestrian,['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and compare rate of pedestrian accidents on the modal day with daily rate for all accidents\n",
    "avg_percent(Pedestrian['Day_of_Week'], 6, weeks, daily_ped_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddd44c",
   "metadata": {},
   "source": [
    "Day of Week with regards to pedestrian accident occurences.\n",
    "\n",
    "The day with the highest occurences of accidents involving pedestrains is Friday, equivalent to the the class with value 6 in the feature. \n",
    "\n",
    "Pedestrian accidents on friday go up to 3859 occurences and constituted 16.66% of total accidents per week. \n",
    "\n",
    "The average number of accidents involving pedestrians that occur on Fridays is less than the daily average (63/day) by 17% (74/friday)\n",
    "\n",
    "\n",
    "The distribution of other occurences across the business days of the week (Monday - Friday) is almost evenly distributed. \n",
    "\n",
    "Similar to the distribution of all accidents, We can assume that Friday being the transition of the week into weekends and the associated commute is responsible for the increased frequency of accidents on that day.\n",
    "\n",
    "The days of the weekend, witnesses relatively less accidents because traffic is lighter as a result of less activities requiring the use of major roads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b3b06",
   "metadata": {},
   "source": [
    "# D. Impact of daylight savings on road traffic accidents\n",
    "\n",
    ">Daylight savings In the Uk in 2019 started on the 31st of March, and ended on the 27th of Octobe of 2019. ([time and date.com][])\n",
    "\n",
    "\n",
    "### What impact does daylight savings have an impact on the occurence of accidents?\n",
    "\n",
    "If our hypothesis is that Daylight savings have an impact on the accident occurences, The distribution of accidents by defined periods of the year will reveal some obvious trend during the first few days of daylight savings. \n",
    "\n",
    "\n",
    "We may test the significance of this assumption.\n",
    "\n",
    "H0 = Daylight savings has no impact on the occurence of accidents.\n",
    "\n",
    "H1 = Daylight savings has an impact on the occurence of accidents.\n",
    "\n",
    "\n",
    "[time and date.com]: https://www.timeanddate.com/time/change/uk/london?year=2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff92eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_test(data, col, test) :\n",
    "    \"\"\"\n",
    "Function to perform Statistical tests of normality and/or parametric tests on a dataframe.\n",
    "\n",
    "It accepts a list of dataframes, \n",
    "the related data column and,\n",
    "the specific test to be performed.\n",
    "\n",
    "There are three tests considered in this function;\n",
    "\n",
    "ttest == student T-test to compare the mean of two groups in a distribution. \n",
    "shapiro == frequency statistic test to determine if data (feature in this case) is normally distributed.\n",
    "ztest == test of mean of two different normal distributions\n",
    "\n",
    "    \"\"\"\n",
    "    if test.lower() == 'shapiro': # Condition for shapiro\n",
    "        shapiro_test,pval = shapiro(data[col]) # Apply shapiro \n",
    "        print(\"p-value for significance is: \", pval)# print the p-value\n",
    "        if pval <0.05:\n",
    "            return \"Reject null hypothesis for shapiro\"# if the value less than the threshold P_value\n",
    "        else:\n",
    "            return \"Accept null hypothesis for shapiro\"# if the  P_value is greater than threshold P_value\n",
    "        \n",
    "    elif test.lower() == 'ttest' : # Check condition for ttest\n",
    "        ttest_test,pval = ttest_ind(data[0][col],data[1][col]) # Apply 2-sample ttest\n",
    "        print(\"p-value for ttest significance is: \", pval) # print the p-value\n",
    "        if pval <0.05: # Condition for p_values \n",
    "            return \"Reject null hypothesis for the T-test\" # if the value less than the threshold P_value\n",
    "        else: \n",
    "            return \"Accept null hypothesis for the T-test\" # if the  P_value is greater than threshold P_value\n",
    "        \n",
    "    elif test.lower() == 'ztest':# Condition for ztest\n",
    "        ztest_test ,pval = stests.ztest(data[0][col], x2=data[1][col], value=0,alternative='two-sided')# Apply z_score\n",
    "        print(\"p-value for ztest significance is: \", pval) # print the p-value \n",
    "        if pval<0.05:\n",
    "            return \"reject null hypothesis for ztest\" # if the value less than the threshold P_value\n",
    "        else:\n",
    "            return \"accept null hypothesis for ztest\" # if the  P_value is greater than threshold P_value\n",
    "        \n",
    "    else :\n",
    "        return \"The test you have selected is not available\" # Check for wrong choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9604f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function performs a non parametric test; Wilcoxon Rank-Sum test (aka mann-withney-u)\n",
    "mannwhitenyu == nonparametric test of equal probabilities between two independent distributions\n",
    "parameters are a list of dataframes and the associated column\n",
    "'''\n",
    "def non_param(data, col):\n",
    "    mannwhitneyu_test,pval = mannwhitneyu(data[0][col], data[0][col])# Apply manwhitneyu \n",
    "    print(\"p-value for mannwhitneyu significance is: \", pval) # print the p-value \n",
    "    if pval <0.05:\n",
    "        return(\"Reject null hypothesis for mannwhitneyu\") # if the value less than the threshold P_value\n",
    "    else:\n",
    "        return(\"Accept null hypothesis for mannwhitneyu\") # if the  P_value is greater than threshold P_value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualize the distribution'''\n",
    "\n",
    "# Plotting for the distribution of accidents across all weeks\n",
    "acc_wk = sns.histplot(data= Accidents, x ='week', kde = True, Color = 'teal')\n",
    "plt.title('Distribution of Accidents across all weeks in 2019')\n",
    "plt.show()\n",
    "figs.append(acc_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a303d",
   "metadata": {},
   "source": [
    "The overall distribution of accidents across all weeks of the year does not appear normal at all. We also observe major spikes in the 1st, Last and the 27th weeks of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8764ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualize the distribution'''\n",
    "\n",
    "# Plotting for the distribution of accidents across all weeks\n",
    "acc_day = sns.histplot(data= Accidents, x ='Day_of_Week', kde = True, Color = 'teal')\n",
    "plt.title('Distribution of Accidents by day of week in 2019')\n",
    "plt.show()\n",
    "figs.append(acc_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f507f4",
   "metadata": {},
   "source": [
    "### 1week into daylight savings\n",
    "\n",
    "Initial analysis is based on observed difference in occurences the week immediately after the start of daylight savings and the rest of the year excluding the week immediately Daylight savings ends.\n",
    "\n",
    "We test our hypotheses by comparing the distribution of occurences across days in both time periods.\n",
    "\n",
    "however, we must confirm that both distributions data meet all statistical requirements including normality in order to perform a parametric test of comparison, otherwise we select a non-parametric test.\n",
    "\n",
    "Tests included in the experiment are because the distributions to be compared have quantitative and discrete outcomes (count of accidents) and are from independent distributions (different periods).\n",
    "\n",
    "For the statistical tests, p-value is set to 5% confidence. If the test score is greater than or equal to 5%, we accept the null hypothesis. \n",
    "\n",
    "Accepting the null hypothesis implies that there is no special relationship between the distributions we are comparing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d88ba",
   "metadata": {},
   "source": [
    "For the purpose of our analysis, we define the weeks immediately after the start and end of the daylight savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = \"%Y-%m-%d\"\n",
    "start_date=datetime.datetime.strptime(\"2019-03-31\", FORMAT) # start of daylight savings\n",
    "\n",
    "end_date=datetime.datetime.strptime(\"2019-10-27\", FORMAT) # end of daylight savings\n",
    "\n",
    "day7_after_start = start_date + datetime.timedelta(weeks=1) # a week from the start date\n",
    "\n",
    "day7_after_end = end_date + datetime.timedelta(weeks=1) # a week from the end date\n",
    "\n",
    "\n",
    "# define all three periods \n",
    "\n",
    "week_after_start = Accidents.loc[(Accidents['Date'] > start_date) & (Accidents['Date'] <= day7_after_start)]\n",
    "\n",
    "week_after_end = Accidents.loc[(Accidents['Date'] > end_date) & (Accidents['Date'] <= day7_after_end)]\n",
    "\n",
    "\n",
    "other_weeks1 = Accidents.loc[(~Accidents['Date'].isin(week_after_start)) & (~Accidents['Date'].isin(week_after_end))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48946816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''get the occurences within both periods for comparison.'''\n",
    "\n",
    "# The Dataframe of accidents per day the week after Daylight Savings started \n",
    "week_after = week_after_start.groupby('Day_of_Week').size().reset_index(name='Accidents')\n",
    "\n",
    "# The Dataframe of accidents per day for other weeks in the year \n",
    "other_weeks = other_weeks1.groupby('Day_of_Week').size().reset_index(name='Accidents')\n",
    "week_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6937a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 =pd.DataFrame({'Day_of_Week':[1, 4], 'Accidents':[0, 0]})\n",
    "week_after = week_after.append(new_data1, ignore_index = True)# include days with no incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We visualize the distribution of accidents in both periods for observed differences'''\n",
    "\n",
    "# Put Sunday first because that is when the time changes\n",
    "af_ds = sns.barplot(data= week_after, x='Day_of_Week', y='Accidents', color = 'navy')\n",
    "#ax = sns.histplot(data= week_after, x ='Day_of_Week', kde = True, Color = 'teal')\n",
    "\n",
    "af_ds.set_xticklabels([\"Sunday (DST)\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], size=14, rotation= 90)\n",
    "plt.title('Accidents in the week after the start of Daylight Savings in the UK(2019))', y=1.03, size=10)\n",
    "plt.show()\n",
    "figs.append(af_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4772027",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_wks = sns.barplot(data= other_weeks, x='Day_of_Week', y='Accidents', color= 'black')\n",
    "other_wks.set_xticklabels([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], size=14, rotation= 90)\n",
    "plt.title('distribution density of accidents by day across other weeks of the year in the UK(2019))', y=1.03, size=10)\n",
    "plt.show()\n",
    "figs.append(other_wks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test normality of distribution and compare distribution of accidents within both weeks\n",
    "def compare_dists(dfs, col):\n",
    "\n",
    "  # confirm normality\n",
    "    outcome = []\n",
    "    for df in dfs:\n",
    "        outcome.append(stats_test(df, col, 'shapiro'))\n",
    "    print(outcome)\n",
    "    if outcome[0] == 'we accept null hypothesis for shapiro' and outcome[1] == 'we accept null hypothesis for shapiro':\n",
    "        print ('Both distributions are Gaussian')\n",
    "        text=input('select a parametric test')# ask to define parametric test\n",
    "        # Now testing hypothesis\n",
    "        result = non_param(dfs,col,text)\n",
    "        return result\n",
    "    else: # if at least one distribution is not normal\n",
    "        print('one or both of the distributions are not Gaussian, running Wilcoxon Rank-Sum test ') \n",
    "        #text=input('select a non-parametric test') # ask to select a non_parametric test\n",
    "        # now testing hypothesis\n",
    "        result = non_param(dfs, col)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([week_after, other_weeks], 'Accidents') #hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e3ad3",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The P-value is greater than 0.05, we therefore accept the null hypothesis and infer that Daylight savings has no impact on the occurence of accidents in the week immediately after it starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3d445",
   "metadata": {},
   "source": [
    "## 1week after daylight savings\n",
    "\n",
    "We compare and repeat the hypothesis test for the week after daylight savings ends and other weeks of the year excluding the weeks immediately after daylight savings ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146626bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get the occurences within both periods for comparison.'''\n",
    "\n",
    "# The Dataframe of accidents per day the week after Daylight Savings ended\n",
    "week2_after = week_after_end.groupby('Day_of_Week').size().reset_index(name='Accidents')\n",
    "\n",
    "week2_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data2 = pd.DataFrame({'Day_of_Week':[1, 7],'Accidents':[0, 0]})\n",
    "week2_after = week2_after.append(new_data2, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1182c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We visualize the distribution of accidents in both periods for observed differences'''\n",
    "\n",
    "# Put Sunday first because that is when the time changes\n",
    "af_wk2 = sns.barplot(data= week2_after, x='Day_of_Week', y='Accidents', color= 'navy')\n",
    "af_wk2.set_xticklabels([\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"], size=14, rotation= 90)\n",
    "plt.title('Accidents in the week after the end of Daylight Savings in the UK(2019))', y=1.03, size=10)\n",
    "plt.show()\n",
    "figs.append(af_wk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([week2_after, other_weeks], 'Accidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf385e0",
   "metadata": {},
   "source": [
    "We also confirm that there is no significant difference in the distribution of accidents in the week after daylight savings ended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c684fe8",
   "metadata": {},
   "source": [
    "# Further investigation \n",
    "\n",
    "1. \"Chi-by-eye\"\n",
    "2. Descriptive statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(week_after_start, ['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62237683",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(week_after_end, ['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(other_weeks1, ['Day_of_Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accidents occured at the rate of {round(rate(len(week_after_start),7))} per day in the week after DS started' )\n",
    "\n",
    "print(f'accidents occured at the rate of {round(rate(len(week_after_end),7))} per day in the week after DS ended')\n",
    "\n",
    "print(f'accidents occured at the rate of {round(rate(len(Accidents), 365))} per day in all other weeks of the year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7b29e",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "from the statistical tests; \n",
    "\n",
    "**Shapiro**\n",
    "We accept H1 and confirm that all distributions are not normal and therefore proceed with non-parametric tests\n",
    "\n",
    "**Manwithneyu**\n",
    "We also ascertain the confidence of probability between occurences in both periods confirming no difference in observations after the start and end of daylight savings.\n",
    "\n",
    "\n",
    "\n",
    "Comparing descriptive statistics of the distributions;\n",
    "\n",
    "The impact of daylight savings on the frequency of accidents is insignificant.\n",
    "\n",
    "2273 accidents happened in the week immediately after daylight savings compared to the weekly average of 2260 for the whole of the year with 32.4% of those happening on Thursdays. The average per day was also 325 accidents compared to 322 accidents per day in the year. \n",
    "\n",
    "In the week after daylight savings ended, a total of 2275 accidents occured compared to the 2260 for all weeks in the year with 41.6% of them happening on Tuesdays. The average per day was 325 accidents compared to 322 across other weeks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725db1e",
   "metadata": {},
   "source": [
    "## E. Impact of sunrise and sunset time on road accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02178703",
   "metadata": {},
   "source": [
    "### What impact, if any, does sunrise and sunset times have on road traffic accidents?\n",
    "\n",
    ">since sunrise and sunset times vary across the year in the uk. we will investigate by group months of the year.\n",
    "The darkest months(with shorter daylight hours)are January, February, October, November and December with less than 11 hours of daylight on the average. The longest days are between March and September.\n",
    "\n",
    "long_days = Mar - sept\n",
    "\n",
    "short_days = Oct - Feb\n",
    "\n",
    "According to ([world data(2019)][])\n",
    "\n",
    "sunrise across the UK during long_days was between 7:12AM(**7.12**) hours and 7:22AM(**7.22**)hrs. \n",
    "\n",
    "sunrise across the UK during short_days was between 4:40AM(**4.40**) hours and 6:33AM(**6.33**)hrs.\n",
    "\n",
    "sunset across the UK during long_days was between 6:06PM(**18.06**) hours and 9:21PM(**21.21**)hrs.\n",
    "\n",
    "sunset across the UK during short_days was between 3:53PM(**15.53**) hours and 6:09PM(**18.09**)hrs.\n",
    "\n",
    "\n",
    "Using similar methods as defined before, we investigate the impact of sunrise and sunset on the occurences of accidents by comparing daylight hours with these periods\n",
    "\n",
    "This is the basis of performing frequentist statistics and creation of dataframes.\n",
    "\n",
    "\n",
    "[world data(2019)]: https://www.worlddata.info/europe/united-kingdom/sunset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4378e",
   "metadata": {},
   "source": [
    "#### Hypothesis testing\n",
    "For:\n",
    "The darkest months\n",
    "\n",
    "The brightest months\n",
    "\n",
    "H0 = Sunrise and sunset have no impact on the occurence of accidents\n",
    "\n",
    "H1 = Sunrise and sunset have an impact on the occurence of accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define both periods\n",
    "\n",
    "brighter_months = Accidents.loc[(Accidents.Month >2) | (Accidents.Month <= 9)]\n",
    "\n",
    "dark_months = Accidents.loc[(Accidents.Month >9) | (Accidents.Month <= 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403048f9",
   "metadata": {},
   "source": [
    "### During the darkest months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab399be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sunrise and sun set times in darkest months\n",
    "rise_dark_months = dark_months.loc[(dark_months.decimal_time >=4.40) & (dark_months.decimal_time <=6.33 )]\n",
    "set_dark_months =dark_months.loc[(dark_months.decimal_time >=15.53) & (dark_months.decimal_time <=18.09 )]\n",
    "\n",
    "#normal daylight period\n",
    "day_time_dm = dark_months.loc[(dark_months.decimal_time >6.33) & (dark_months.decimal_time <15.53)]\n",
    "\n",
    "'''Create dataframes'''\n",
    "\n",
    "# accidents per hour during sunrise in the darkest months\n",
    "sunrise_dm = rise_dark_months.groupby('decimal_time').size().reset_index(name='Accidents')\n",
    "\n",
    "# accidents per hour during sunset in the dark months\n",
    "sunset_dm = set_dark_months.groupby('decimal_time').size().reset_index(name='Accidents')\n",
    "\n",
    "# accidents per hour during sunset in the dark months\n",
    "daytime_dm = day_time_dm.groupby('decimal_time').size().reset_index(name='Accidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([daytime_dm, sunrise_dm], 'Accidents') #for sunrise in brigher months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([daytime_dm, sunset_dm], 'Accidents') #for sunset in brigher months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815117c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrise_dm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during sunrise in the dark months\n",
    "percentage_rate(len(rise_dark_months), len(dark_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ee093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during sunrise. There are exactly 1.88hrs (giga calculator) between 4.40 and 6.33am= 1.88 x 151 (no of days in the dark months)\n",
    "rate(len(rise_dark_months), (1.88*151))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_dm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26735f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during sunset in the dark months\n",
    "percentage_rate(len(set_dark_months), len(dark_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during sunset. There are exactly 2.27hrs(giga calculator) between 15.53 and 18.09pm= 2.27 x 151 (no of days in the dark months)\n",
    "rate(len(set_dark_months), (2.27*151))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b050e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_dm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during normal daylight hours in the dark months\n",
    "percentage_rate(len(day_time_dm), len(dark_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during normal daylight hours. \n",
    "#There are exactly 9.33 hours(giga calculator) between 6.33am and 15.53pm= 9.33 x 151 (no of days in the dark months)\n",
    "rate(len(day_time_dm), (9.33*151))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "dark = sns.kdeplot(data= dark_months, x='decimal_time')\n",
    "plt.axvline(x=4.40, linewidth=1.5, color=\"red\", linestyle=\"dashed\")\n",
    "plt.axvline(x=6.33, linewidth=1.5, color=\"green\", linestyle=\"dashed\")\n",
    "plt.axvline(x=15.53, linewidth=1.5, color=\"blue\", linestyle=\"dashed\")\n",
    "plt.axvline(x=18.09, linewidth=1.5, color=\"0.25\", linestyle=\"dashed\")\n",
    "plt.title('Distribution of Accidents around sunrise and sunset in the darkest months ', fontdict={'size':15})\n",
    "plt.xlabel('Time of Day')\n",
    "plt.show()\n",
    "figs.append(dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc728b7",
   "metadata": {},
   "source": [
    "### During the brightest months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc642d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sunrise and sun set times in brighter months\n",
    "rise_brighter_months = brighter_months.loc[(brighter_months.decimal_time >=7.12) & (brighter_months.decimal_time <=7.22 )]\n",
    "set_brighter_months = brighter_months.loc[(brighter_months.decimal_time >=18.06) & (brighter_months.decimal_time <=21.21 )]\n",
    "\n",
    "\n",
    "#normal daylight period\n",
    "day_time_bm = brighter_months.loc[(brighter_months.decimal_time >7.22) & (brighter_months.decimal_time <18.06)]\n",
    "\n",
    "\n",
    "'''Create dataframes'''\n",
    "\n",
    "# accidents per hour during sunrise in the bright months\n",
    "sunrise_bm = rise_brighter_months.groupby('decimal_time').size().reset_index(name='Accidents')\n",
    "\n",
    "\n",
    "# accidents per hour during sunset in the bright months\n",
    "sunset_bm = set_brighter_months.groupby('decimal_time').size().reset_index(name='Accidents')\n",
    "\n",
    "\n",
    "# accidents per hour during sunset in the bright months\n",
    "daytime_bm = day_time_bm.groupby('decimal_time').size().reset_index(name='Accidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([sunset_bm, daytime_bm], 'Accidents') #for sunset in the brightest months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dists([sunrise_bm, daytime_bm], 'Accidents') #for sunrise in the brightest months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a9c90",
   "metadata": {},
   "source": [
    "# Further investigation \n",
    "\n",
    "1. Descriptive statistics\n",
    "2. \"Chi-by-eye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12decb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrise_bm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during sunrise in the brighter months\n",
    "percentage_rate(len(rise_brighter_months), len(brighter_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74312a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during sunrise. There are exactly 0.17hrs (giga calculator) between 7.12 and 7.22am= 0.17 x 214 (no of days in the brighter months)\n",
    "rate(len(rise_brighter_months), (0.17*214))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328558ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_bm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during sunset in the brighter months\n",
    "percentage_rate(len(set_brighter_months), len(brighter_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b458107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during sunset. There are exactly 3.25hrs(giga calculator) between 18.06 and 21.21pm= 3.25 x 214 (no of days in the dark months)\n",
    "rate(len(set_brighter_months), (3.25*214))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime_bm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1667d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage rate of accidents during normal_daylight hours in the dark months\n",
    "percentage_rate(len(day_time_bm), len(brighter_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c31be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate per hour during normal daylight hours. \n",
    "#There are exactly 10.73 hours(giga calculator) between 7.22am and 18.06pm= 10.73 x 214 (no of days in the dark months)\n",
    "rate(len(day_time_bm), (10.73*214))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59467ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "bright = sns.kdeplot(data= brighter_months, x='decimal_time')\n",
    "\n",
    "plt.axvline(x=7.12, linewidth=1.5, color=\"red\", linestyle=\"dashed\")\n",
    "plt.axvline(x=7.22, linewidth=1.5, color=\"green\", linestyle=\"dashed\")\n",
    "plt.axvline(x=18.06, linewidth=1.5, color=\"blue\", linestyle=\"dashed\")\n",
    "plt.axvline(x=21.21, linewidth=1.5, color=\"0.25\", linestyle=\"dashed\")\n",
    "plt.title('Distribution of Accidents around sunrise and sunset in the brightest months ', fontdict={'size':15})\n",
    "plt.xlabel('Time of day')\n",
    "plt.show()\n",
    "figs.append(bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a9e9c",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Sunrise and sunset have an impact on the occurence of accidents. \n",
    "\n",
    "\n",
    "It is important to note that although sunrise and sunset times vary across months in the year, the distribution of occurences across hours of the day remains consistent for both periods(darkest and brightest months) as is observable from the plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264149c",
   "metadata": {},
   "source": [
    "# F. Relationship of vehicle characteristics and occurences of Accidents.\n",
    "\n",
    "### Are there particular types of vehicles (engine capacity, age of vehicle, etc.) that are more frequently involved in road traffic accidents?\n",
    "\n",
    "1. Descriptive statistics\n",
    "2. Chi_by_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e1595",
   "metadata": {},
   "outputs": [],
   "source": [
    " #################################### Quantitative features #######################################################\n",
    "Get_stats(Analysis,['Age_of_Vehicle','Engine_Capacity_(CC)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d94dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_features = Analysis[['Age_of_Vehicle','Engine_Capacity_(CC)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc23ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in veh_features.columns:\n",
    "    #display boxplots\n",
    "    #sns.set_theme(style=\"white\")\n",
    "    sns.boxplot(data=veh_features, x=i, color = 'brown')\n",
    "    plt.title(f'distribution of accidents in vehicles based on the {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9945f",
   "metadata": {},
   "source": [
    "The boxplot shows the presence of outliers in the distribution of accidents accorcing to Vehicle engine capacity. \n",
    "Outliers must be handled to make meaningful statistical deductions about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ec250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fix_outliers(data, feature, method, upper = None, lower = None, impute = None):\n",
    "    \"\"\"\n",
    "        Function handles outliers using various methods. \n",
    "        It accepts a dataframe, a dataframe column, a method, \n",
    "        an upper percentile score- none by default,\n",
    "        a lower percentile score - none by default\n",
    "        impute - none by default \n",
    "        as parameters. \n",
    "\n",
    "        There are three methods defined in this function :\n",
    "        Imputation = replacing outliers with a constant value from the distribution\n",
    "        Capping = replacing outliers with the upper and lower percentile values\n",
    "        Log_transform = transforming the column values to logrithmic values\n",
    "    \"\"\"\n",
    "    if method.lower() == 'imputation': #Checking condition for imputation\n",
    "        Q1 = data[feature].quantile(lower) # Storing value for upper limit\n",
    "        Q3 = data[feature].quantile(upper) # Storing values for lower limit\n",
    "        print(data[feature].skew()) # Printing the skewness of the feature\n",
    "        data[feature] = np.where(data[feature] < Q1, impute, data[feature] ) #Replace outliers with an imputted value and assign it to the feature,lower limit\n",
    "        data[feature] = np.where(data[feature] > Q3, impute, data[feature] )# Replace outlierswith an imputted and assign it to feature, upper limit\n",
    "        print(df[feature].skew())# Printing the skewness of the feature afterwards\n",
    "        return data[feature] # Return the fixed feature\n",
    "    elif method.lower() == 'capping': # Check condition for capping\n",
    "        Q1 = data[feature].quantile(lower) #Assign value for lower limit\n",
    "        Q3 = data[feature].quantile(upper) # Assign value for upper limit\n",
    "        print(data[feature].skew()) # Print the skewness of the feature\n",
    "        data[feature] = np.where(data[feature] < Q1, Q1, data[feature] ) #Replace outliers with the lower limit value and assign it to the feature\n",
    "        data[feature] = np.where(data[feature] > Q3, Q3, data[feature] ) # Replace outliers with the upper limit value and assign it to the feature\n",
    "        print(data[feature].skew()) # Print the skewness of the feature\n",
    "        return data[feature] #Return the fixed feature\n",
    "    elif method.lower() == 'log_transform': # Checking condition for log_transform\n",
    "        print(data[feature].skew()) #Print skewness of the feature\n",
    "        data[feature] = np.log(data[feature]) + 1 # Performing the log transformation\n",
    "        print(data[feature].skew()) # print the skewness after fixing\n",
    "        return data[feature] # Return the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_features['Age_of_Vehicle'] = Fix_outliers(veh_features, 'Age_of_Vehicle', 'capping', 0.95, 0.05)\n",
    "\n",
    "veh_features['Age_of_Vehicle'] = Fix_outliers(veh_features, 'Engine_Capacity_(CC)', 'capping', 0.95, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in veh_features.columns:\n",
    "    sns.boxplot(data=veh_features, x=i, color = 'brown')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0eedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis.Age_of_Vehicle.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad0f46",
   "metadata": {},
   "source": [
    "Investigating the frequency of accidents based on invidual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f43ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Age of Vehicle #####################################################\n",
    "# Bin age of vehicle to simplify analysis and generate column\n",
    "Analysis['Age_of_Vehicle'] = pd.cut(x=df['Age_of_Vehicle']\n",
    "                         , bins=[-1, 0, 5, 10,  20, 40, 70, 94 ]\n",
    "                         , labels=['unknown','1-5','6-10','11-20','21-40','40-70','70+'], include_lowest=True)\n",
    "\n",
    "Age_of_Vehicle = Analysis.groupby('Age_of_Vehicle').size().reset_index(name='Accidents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b706ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#sns.set_theme(style=\"white\")\n",
    "\n",
    "\n",
    "Age_of_Vehicle.plot(x = 'Age_of_Vehicle', kind='barh', ax=ax, cmap='PRGn_r')\n",
    "ax.set_title('\\nDistribution of accidents based on Age of vehicle\\n', fontsize=14, fontweight='bold')\n",
    "ax.set(xlabel='frequency', ylabel='')\n",
    "ax.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True);\n",
    "figs.append(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50936f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_of_Vehicle['%'] = Age_of_Vehicle[[\"Accidents\"]].apply(lambda x: 100*x/x.sum())\n",
    "Age_of_Vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3239716",
   "metadata": {},
   "source": [
    "'The size – or cubic capacity – of a Vehicle’s engine is measured in cubic centimetres (cc). It refers to the amount of air and fuel that can be pushed through the cylinders in the engine. In most cases, the general rule of thumb is that the bigger the capacity, the more powerful it tends to be' [Cinch][]. \n",
    "\n",
    "Safe limits for motorbikes are between 200 & 500CC and the recommended maximum capacity for any vehicle that is at least 1100kg and naturally aspirated is its Original mass (kg) x 5.0 [Street Machine][]. The minimum for the lowest motorcycle category test is 120cc [ndirect.gov.uk][] and for other vehicles is 1000 cc [MCNally Institute][] on the average. \n",
    "\n",
    "Based on this knowledge, we may group engine capacities by power for easier analysis setting the treshold at 7500CCs for vehicles other than motorcycles\n",
    "\n",
    "[Cinch]: https://www.cinch.co.uk/jargon/cc-cubic-capacity#:~:text=The%20size%20%E2%80%93%20or%20cubic%20capacity,powerful%20it%20tends%20to%20be.\n",
    "[Street Machine]: https://www.whichcar.com.au/features/diy/how-to-work-out-the-largest-road-legal-engine-size-for-your-street-machine\n",
    "[ndirect.gov.uk]: https://www.nidirect.gov.uk/articles/minimum-requirements-test-vehicles\n",
    "[MCNally Institute]: https://www.mcnallyinstitute.com/what-percentage-of-the-uk-are-over-3l-engine-cars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Engine capacity #####################################################\n",
    "#map other vehicle types.\n",
    "vehicles = {1:'Pedal cycle',8:'Taxi/Private car',9:'car',10:'Minibus',\n",
    "             11:'Bus/Coach',16:'Ridden horse',17:'Agric Vehicle',\n",
    "             18:'Tram',19:'Van/Goods 3.5t',\n",
    "             21:'Goods 7.5 tonnes mgw and over',20:'Goods over 3.5t. and under 7.5t',\n",
    "             90:'Other vehicle', 98:'other Goods Vehicle',-1:'missing'}\n",
    "Analysis['Vehicle_Type'] = Convert(Analysis,'Vehicle_Type', vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89847265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for motorcycles\n",
    "#update dataframe\n",
    "Motorcycle = Analysis.loc[Analysis['Vehicle_Type'].isin(m_values)]\n",
    "\n",
    "#bin engine capacity for motorcycles \n",
    "Motorcycle['Engine_Capacity_range'] = pd.cut(x=Motorcycle['Engine_Capacity_(CC)']\n",
    "                         , bins=[-1, 0, 50, 610, 7817 ]\n",
    "                         , labels=['unknown','low','good range','very high'], include_lowest=True)\n",
    "\n",
    "\n",
    "# for non motorcycles \n",
    "non_two_wheeled = Analysis.loc[~(Analysis['Vehicle_Type'].isin(m_values))]\n",
    "\n",
    "#bin engine capacity for other vehicles\n",
    "non_two_wheeled['Engine_Capacity_range'] = pd.cut(x=Analysis['Engine_Capacity_(CC)']\n",
    "                         , bins=[-1, 0, 1000, 7500, 29980 ]\n",
    "                         , labels=['unknown','low','good range','very high'], include_lowest=True)\n",
    "\n",
    "\n",
    "\n",
    "#vehicle_cc = non_two_wheeled.groupby('Engine_Capacity_range').size().reset_index(name='Accidents')\n",
    "\n",
    "#Motorcycle_cc = Motorcycle.groupby('Engine_Capacity_range').size().reset_index(name='Accidents')\n",
    "\n",
    "# define all vehicle types excluding motorcycles\n",
    "non_two_wheeled['Vehicle_cat'] = non_two_wheeled['Vehicle_Type']\n",
    "\n",
    "#cast all motorcycles to class 'motorcycle'\n",
    "Motorcycle['Vehicle_cat'] = 'Motorcycle'\n",
    "\n",
    "#get all vehicles\n",
    "all_vehicles = pd.concat([non_two_wheeled, Motorcycle])\n",
    "\n",
    "        \n",
    "all_vehicles['Vehicle_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define figure size\n",
    "plt.figure(figsize=(6,5))\n",
    "#sns.set_theme(style=\"white\")\n",
    "#display countplot\n",
    "all_veh = sns.countplot(data = all_vehicles\n",
    "            ,x = 'Engine_Capacity_range'\n",
    "            #,hue = 'Vehicle_cat'\n",
    "            ,color='orange'\n",
    "            )\n",
    "plt.xlabel('Engine_capacity_range', size=16)\n",
    "plt.ylabel('frequency', size=16)\n",
    "plt.title('Distribution of Accidents based on vehicle engine capacity', size=18)\n",
    "plt.show()\n",
    "figs.append(all_veh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_rate(len(all_vehicles[all_vehicles['Engine_Capacity_range']=='unknown']), len(all_vehicles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d59c3",
   "metadata": {},
   "source": [
    "From the analysis, Vehicles with ok engine capacity account for the highests occurences of accidents. Vehicles with engine capacity in unknown are the second largest group involved in accidents. Vehicles whose engine capacities are unknown also account for about 21.8% of all vehicles invoved in accidents in 2019.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0393af",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Propulsion code ############################################\n",
    "\n",
    "# map values to appropriate vehicle propulsion class\n",
    "\n",
    "propulsion = {1:'Petrol',2:'Heavy',3:'Electric',4:'Steam',\n",
    "             5:'Gas',6:'Petrol/Gas(LPG)',7:'Gas/Bi-fuel',\n",
    "             8:'Hybrid electric',9:'Gas Diesel',\n",
    "             10:'New fuel technology',11:'Fuel cells',\n",
    "             12:'Electric diesel','M':'Undefined',-1:'missing'}\n",
    "Analysis['Propulsion_Code'] = Convert(Analysis,'Propulsion_Code', propulsion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Analysis, ['Propulsion_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830bc47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the distribution of accidents according to vehicle propulsion codes\n",
    "prop_code = sns.countplot(data= Analysis, y='Propulsion_Code', color ='teal')\n",
    "plt.title(\"distribution of accidents by vehicle propulsion codes\", size = 14)\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.show()\n",
    "figs.append(prop_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fce2a",
   "metadata": {},
   "source": [
    "The analysis carried out reveals that vehicles whose propulsion code is Petrol are the most likely to get involved in accidents with their such vehicles involved in 45.41% of all accidents.\n",
    "\n",
    "*NB.* The available data indicates that there is a problem with identifying and collecting vehicle feature information for vehicles involved in road traffic accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940deef",
   "metadata": {},
   "source": [
    "# G. Relationship between Environmental conditions and occurence of Accidents\n",
    "\n",
    "### Are there particular conditions (weather, geographic location, situations) that generate more road traffic accidents?\n",
    "\n",
    "It would seem a fair assumption that driving in poor weather conditions and low light may increase the risk of accidents. \n",
    "We investigate to confirm this assumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92dba7",
   "metadata": {},
   "source": [
    "### Weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Investigating accidents during varying weather ################################## \n",
    "# Convert feature values to weather conditions\n",
    "weather = {1:'Fine no high winds',2:'Raining no high winds',3:'Snowing no high winds',4:'Fine + high winds',\n",
    "             5:'Raining + high winds',6:'Snowing + high winds',7:'Fog or mist',\n",
    "             8:'Other',9:'Unknown',\n",
    "             -1:'missing'}\n",
    "Accidents['Weather_Conditions'] = Convert(Accidents,'Weather_Conditions', weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define figure size\n",
    "plt.figure(figsize=(6,6))\n",
    "#sns.set_theme(style= 'white')\n",
    "#display countplot\n",
    "w_cond = sns.countplot(y=\"Weather_Conditions\", data=Accidents, palette=\"PRGn_r\")\n",
    "plt.ylabel('Weather_Condition', size=16)\n",
    "plt.xlabel('no of accidents', size=16)\n",
    "plt.title('Accidents during various weather conditions', size=18)\n",
    "figs.append(w_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accidents by weather conditions\n",
    "weather = Accidents.groupby('Weather_Conditions').size().reset_index(name='Accidents')\n",
    "Get_stats(Accidents, ['Weather_Conditions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "weather.plot(x = 'Weather_Conditions', kind='barh', ax=ax, cmap='PRGn_r')\n",
    "ax.set_title('\\nDistribution of accidents based on Age of vehicle\\n', fontsize=14, fontweight='bold')\n",
    "ax.set(xlabel='frequency', ylabel='')\n",
    "ax.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717d9ea",
   "metadata": {},
   "source": [
    "From the summary statistics and plot of the distribution we can deduce that most accidents occurred when the weather was fine with no high winds.\n",
    "With the frequency during this period at 92316 accidents accounting for 78.5% of all accidents.\n",
    "\n",
    "We can infer that bad weather conditions have little to no impact on the occurence of accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b9ac2",
   "metadata": {},
   "source": [
    "### Geographic Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Investigating accidents based on Geographic Location ######################################\n",
    "# read in file containing Local Authority district names for matching.\n",
    "LA = pd.read_csv('LA.csv')\n",
    "LA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAs = dict(zip(LA.code, LA.label))\n",
    "LA['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b56f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accidents['Local_Authority_(District)'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b826bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match LA class\n",
    "Accidents['LA'] = Convert(Accidents,'Local_Authority_(District)', LAs)\n",
    "Analysis['LA']= Convert(Analysis,'Local_Authority_(District)', LAs)\n",
    "Accidents['LA'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = Accidents.groupby(['Longitude', 'Latitude', 'LA']).size().reset_index(name='frequency')\n",
    "coord.plot(x=\"Longitude\", y=\"Latitude\", kind=\"scatter\", c=\"frequency\",\n",
    "        colormap=\"YlOrRd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e1592",
   "metadata": {},
   "source": [
    "#### By Local Authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae46894",
   "metadata": {},
   "outputs": [],
   "source": [
    "la =Accidents.groupby('LA').size().reset_index(name=\"Accidents\")\n",
    "\n",
    "la = la.sort_values(['Accidents'], ascending=False)\n",
    "\n",
    "\n",
    "la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e0d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the distribution of accidents according to vehicle propulsion codes\n",
    "la_20 = la[:20].plot(x=\"LA\", y=\"Accidents\", kind=\"barh\",\n",
    "        colormap=\"jet_r\")\n",
    "\n",
    "la_20.set_title('\\nTop 20 local Authorities with the highest frequency of accidents\\n', fontsize=14, fontweight='bold')\n",
    "la_20.set(xlabel='frequency', ylabel='')\n",
    "la_20.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True);\n",
    "figs.append(la_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da2697",
   "metadata": {},
   "source": [
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e19e0",
   "metadata": {},
   "source": [
    "#### By geographic area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Engineer Distance(location_area) Feature from the coordinates using a spatial distance algorithm. \n",
    "To facilitate further analysis'''\n",
    "\n",
    "# we adopt cityblock(manhattan) algorithm because it is appropriate for roads.\n",
    "\n",
    "location_area = [] # Create an empty list\n",
    "for i in range(len(Accidents)): # iterate through dataframe rows\n",
    "    dist = cityblock(Accidents['Longitude'][i],Accidents['Latitude'][i]) # Get the cityblock distance measures\n",
    "    km = 6371*(math.pi/180)*dist # Convert the distance to appropriate SI unit(kilometre)\n",
    "    \n",
    "    location_area.append(km) # Append distances to empty list\n",
    "Accidents['location_area'] = location_area # create  a location_area column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineered feature is scaled to get the difference of the accident location(in km) closest to the null island \n",
    "#(point where the longitude and latitude is equals to zero as reference) from other locations in Kilometre.\n",
    "\n",
    "dist_from_null = Accidents['location_area'] - Accidents['location_area'].min() # Get the difference in area by deducting the minimum value from other values\n",
    "Accidents['dist_from_null'] = dist_from_null # Use the kilometre difference list to create a feature in Analysis dataframe\n",
    "Accidents['dist_from_null'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accidents['dist_from_null'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9208f",
   "metadata": {},
   "source": [
    "The locations with the highest accident occurence in the year 2019 is around 201, 617, 618 qnd 666 kms from our reference point respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accidents['dist_from_null'].value_counts().index.tolist()[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a767a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the dataframe of the locations(Kilometre) within the area of the top 4 locations with highest frequencies\n",
    "risky_sites = Accidents[(Accidents['dist_from_null']== 201.920202) & \n",
    "                        (Accidents['dist_from_null']==617.699048) | (Accidents['dist_from_null']==618.363438) & \n",
    "                        (Accidents['dist_from_null']==666.766145)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d728b",
   "metadata": {},
   "source": [
    "#### By geographic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elbow_method(data, n, init, n_init, max_iter, random_state):\n",
    "    \"\"\"\n",
    "        Function to determine the optimal value of number of clusters(k) in KMeans clustering.\n",
    "        It accepts a dataframe,\n",
    "        range of values, initial_value, number of intialized, maximum iteration and random values\n",
    "    \"\"\"\n",
    "    kmeans_kwargs ={\n",
    "        'init': init,\n",
    "        'n_init': n_init,\n",
    "        'max_iter': max_iter,\n",
    "        'random_state': random_state\n",
    "    } # Initialise Kmeans algorithm\n",
    "    sse = [] # empty list to store number of K\n",
    "    for i in range(1,n): # loop through n \n",
    "        kmeans = KMeans(n_clusters=i, **kmeans_kwargs) # Initialize Kmeans\n",
    "        kmeans.fit(data)# fit in the data\n",
    "        sse.append(kmeans.inertia_) #append the number of inertia\n",
    "    plt.style.use(\"fivethirtyeight\") # Plotting style\n",
    "    plt.plot(range(1,n),sse) # plot graph\n",
    "    plt.xticks(range(1,n)) # x axis range\n",
    "    plt.xlabel(\"Number of Clusters\")  # Label x axis\n",
    "    plt.ylabel(\"SSE\") # Label y axis\n",
    "    plt.show() # show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c878222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use elbow method to determine the optimal number of clusters for Kmeans clustering\n",
    "new_location = Accidents[['Longitude', 'Latitude']]\n",
    "Elbow_method(new_location,15,'random', 10, 300, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bc91d",
   "metadata": {},
   "source": [
    "from math import radians\n",
    "df['Lat'] = df['Latitude'].apply(radians)\n",
    "df['Long'] = df['Longitude'].apply(radians)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=N, metric='haversine')\n",
    "cluster_labels = clusterer.fit_predict(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9484767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster(data, init, n_cluster, n_init, max_iter, random_state ) :\n",
    "    \"\"\"\n",
    "        Function to perform KMeans clustering, a clustering algorithm in Machine Learning.\n",
    "        Cluster algorithms use distance measures.\n",
    "        Function accepts parameters;\n",
    "        dataframe, initial value, number of clusters to form(k), number of initial values, maximum iterations,\n",
    "        random state\n",
    "    \"\"\"\n",
    "    print(data.isnull().sum()) # Confirm data has no missing values\n",
    "    kmeans = KMeans(init = init, n_clusters = n_cluster, n_init = n_init, max_iter = max_iter, random_state = random_state) #Initialize kmean clustering\n",
    "    kmeans.fit(data) # Fit model\n",
    "    return kmeans.labels_ # Return label results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Kmeans clustering to engineer a new feature using its inertia_label as values\n",
    "Accidents['loc_cluster'] = Cluster(new_location, 'random',4,10, 300, 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c199e9f",
   "metadata": {},
   "source": [
    "from math import radians\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#coords = df.as_matrix(columns=['Latitude', 'Longitude'])\n",
    "db = DBSCAN(eps=0.2, min_samples=5, algorithm='ball_tree', metric='haversine').fit_predict(np.radians(new_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d9e3",
   "metadata": {},
   "source": [
    "plt.scatter(X['lat'], X['lng'], c=X['cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['loc_cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45e709",
   "metadata": {},
   "source": [
    "Cluster 2 is the modal class with frequency of 56567 accidents. \n",
    "\n",
    "Cluster 2 constitutes 48.12% of all accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the cluster labels to categorical features\n",
    "Accidents['loc_cluster'] = Accidents['loc_cluster'].astype('category')\n",
    "\n",
    "# plot the accidents distribiution by location clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "clust=sns.scatterplot(data= Accidents, x='Longitude',y='Latitude', hue='loc_cluster') # Plot the graph\n",
    "plt.title('Distribution of accidents in various location clusters')\n",
    "plt.show()\n",
    "figs.append(clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117de5",
   "metadata": {},
   "source": [
    "### Road Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Investigating accidents on various road types ##########################################\n",
    "\n",
    "# Convert Road_Type feature values to road type names\n",
    "road = {1:'Roundabout',2:'One way street',3:'Dual carriage',6:'Single carriageway',7:'Slip road',\n",
    "           9:'Unknown',12:'One way street/Slip road',\n",
    "             -1:'missing'}\n",
    "Accidents['Road_Type'] = Convert(Accidents,'Road_Type', road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cecc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_tp= sns.countplot(data= Accidents, y='Road_Type', color = 'green')\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.title('Accidents on various Road Types in the UK(2019)', size= 14)\n",
    "plt.show()\n",
    "figs.append(road_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Road_Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69db5c",
   "metadata": {},
   "source": [
    "From the summary statistics and plot of the distribution we can deduce that a high number of accidents occurred on Single carriageways With the frequency of occurences at 85320 accidents accounting for 72.6% of all accidents. The increased risk on such roads might be explained by the lack on lane dividers on single carriageways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b61bb",
   "metadata": {},
   "source": [
    "### Lighting conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Investigating accidents with various lighting conditions #####################################\n",
    "\n",
    "# Convert light_Conditions values to light conditions names\n",
    "light = {1:'Daylight',4:'Darkness-lights lit',5:'Darkness - light unlit',6:'Darkness - no lighting',\n",
    "        7:'Darkness - lighting unknown',\n",
    "             -1:'missing'}\n",
    "Accidents['Light_Conditions'] = Convert(Accidents,'Light_Conditions', light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09745eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot light conditions distribution\n",
    "light_cond=sns.countplot(data= Accidents, x='Light_Conditions')\n",
    "plt.tick_params(axis='x', rotation=45)\n",
    "plt.title('Accidents during various lighting conditions')\n",
    "plt.show()\n",
    "figs.append(light_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Light_Conditions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678be60",
   "metadata": {},
   "source": [
    "From the summary statistics and plot of the distribution we can deduce that a high number of accidents occur in normal daylight with the frequency of occurences at 83511 accidents accounting for 71.05% of all accidents. Traffic is busier during the day than at night with more people driving in daylight and directly increasing the risk of accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accae4ec",
   "metadata": {},
   "source": [
    "### Road surface conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Investigating accidents with road surface conditions ######################################\n",
    "\n",
    "#Convert Road_Surface_Conditions feature values to Road_Surface_Conditions names\n",
    "surface = {1:'Dry',2:'Wet or damp',3:'Snow',4:'Frost or ice',5:'Flood over 3cm. deep',\n",
    "           6:'Oil or diesel',7:'Mud',\n",
    "             -1:'missing'}\n",
    "Accidents['Road_Surface_Conditions'] = Convert(Accidents,'Road_Surface_Conditions', surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd36cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for frequencies of road surface conditions\n",
    "surf_cond=sns.countplot(data= Accidents, y='Road_Surface_Conditions', color = 'red')\n",
    "plt.title('Accidents on various Road_Surface_Conditions', size= 14)\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.show()\n",
    "figs.append(surf_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Road_Surface_Conditions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f16f4",
   "metadata": {},
   "source": [
    "70% of all accidents happen on dry roads. Less people drive in bad weather including raining weather as previously discussed explaining this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf4b23",
   "metadata": {},
   "source": [
    "### Special situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d212ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Investigating accidents in special situations ##########################################\n",
    "\n",
    "#Convert values in special conditions on site to special conditions names\n",
    "special = {0:'None',1:'Auto traffic signal-out',2:'Auto signal part defective',3:'Road sign or marking defective or obscured',4:'Roadworks',5:'Road surface defective',\n",
    "           6:'Oil or diesel',7:'Mud',\n",
    "             -1:'missing'}\n",
    "Accidents['Special_Conditions_at_Site'] = Convert(Accidents,'Special_Conditions_at_Site', special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f768a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Special_Condition_at_Site\n",
    "site_cond=sns.countplot(data= Accidents, y='Special_Conditions_at_Site', color = 'green')\n",
    "plt.title('Accidents by site Condition', size = 14)\n",
    "#plt.tick_params(axis='x', rotation=45)\n",
    "plt.show()\n",
    "figs.append(site_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af212ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_stats(Accidents, ['Special_Conditions_at_Site'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a58f34",
   "metadata": {},
   "source": [
    "Less accidents(3.7%) happen at sites with special Conditions than they do or regular roads.Special conditions like defects on the roads are not a norm and as such do not contribute largely to the risk of accidents in comparison with regular roads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eecd32",
   "metadata": {},
   "source": [
    "# H. Driver description and Accidents\n",
    "\n",
    "### How do driver related variables affect the outcome (e.g., age of the driver, and the purpose of the journey)?\n",
    "\n",
    "We examine the relationship between specific factors and the nature of the accident(severity) using apriori knowledge\n",
    "\n",
    "We also plot the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map values to Casualty_Severity class\n",
    "severity = {1:'Fatal',2:'Serious',3:'Slight'}\n",
    "Analysis['Casualty_Severity'] = Convert(Analysis,'Casualty_Severity', severity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ae02e",
   "metadata": {},
   "source": [
    "### Age of Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9892b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin age group and generate column\n",
    "Analysis['Age_group_of_Driver'] = pd.cut(x=df['Age_of_Driver']\n",
    "                         , bins=[-1, 0, 5, 10, 15, 20, 25, 35, 45, 55, 65, 75, 101]\n",
    "                         , labels=['unknown','0-5','6-10','11-15','16-20','21-25','26-35',\n",
    "          '36-45', '46-55','56-65','66-75','75+'], include_lowest=True)\n",
    "Analysis[['Age_group_of_Driver', 'Age_of_Driver']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### investigating relationship of features #################################### \n",
    "\n",
    "def get_dummies(data:pd.DataFrame, feature:list, prefix:list,df_name:list, new_data:str):\n",
    "    \"\"\"\n",
    "    The function is to join two dummied features together, \n",
    "    it accept as parameters; data, feature list, prefix list, df_name list\n",
    "    and new data type.\n",
    "    \"\"\"\n",
    "    df_name[0] = pd.get_dummies(data[feature[0]],prefix=prefix[0])\n",
    "    df_name[1] = pd.get_dummies(data[feature[1]],prefix=prefix[1])\n",
    "    new_data = df_name[0].join(df_name[1])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rule\n",
    "Age_severity = get_dummies(Analysis,['Casualty_Severity','Age_group_of_Driver'],['Severity','Driver'],['sev','drv'],'sev_drv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6293d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apriori_association(data, min_support, min_threshold, metric):\n",
    "    \"\"\"\n",
    "    This function deduces association rules by implementing apriori algorithm.\n",
    "    Parameters are a dataframe, minimum support,\n",
    "    minimum threshold and metrics as parameters\n",
    "    \"\"\"\n",
    "    frequent_set = apriori(data, min_support= min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_set, metric=metric,min_threshold=min_threshold)\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0943e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test relationship\n",
    "Apriori_association(Age_severity, 0.1, 0.5, 'lift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730b462",
   "metadata": {},
   "source": [
    "The result is two associations with high support and confidence for age group of driver and severity of casualty with support of 0.1 and confidence of 0.9. However the relationship is for drivers whose age groups are unknown.\n",
    "\n",
    "we confirm a relationship between the age group of drivers and the severity of casualties involved in the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d23bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group to examine impact\n",
    "\n",
    "by_age = Analysis.groupby(['Age_group_of_Driver', 'Casualty_Severity']).size()\n",
    "\n",
    "by_age = by_age.unstack('Casualty_Severity')\n",
    "\n",
    "by_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf61eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize relationship\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "by_age.plot(kind='barh', ax=ax, stacked=True, cmap='copper')\n",
    "ax.set_title('\\nAccident Severity by Age group of driver\\n', fontsize=14, fontweight='bold')\n",
    "ax.set(xlabel='frequency', ylabel='')\n",
    "ax.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a05b7",
   "metadata": {},
   "source": [
    "### Purpose of journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map values to Journey_Purpose_of_Driver class\n",
    "\n",
    "Purpose = {1:'Journey as part of work',2:'Commuting to/from work',3:'Taking pupil to/from school',4:'Pupil riding to/from school',5:'Other',\n",
    "           6:'Not known',7:'Other/Not known (2005-10)',\n",
    "             -1:'Data missing or out of range'}\n",
    "Analysis['Journey_Purpose_of_Driver'] = Convert(Analysis,'Journey_Purpose_of_Driver',Purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rule\n",
    "Journey_casualty = get_dummies(Analysis, ['Journey_Purpose_of_Driver','Casualty_Severity'],['purpose','severity'],\n",
    "                                    ['pur','sev'],'pur_sev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test relationship\n",
    "Apriori_association(Journey_casualty,0.1,0.5,'lift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group to examine impact\n",
    "\n",
    "by_purpose = Analysis.groupby(['Journey_Purpose_of_Driver', 'Casualty_Severity']).size()\n",
    "\n",
    "by_purpose = by_purpose.unstack('Casualty_Severity')\n",
    "\n",
    "by_purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d34d8",
   "metadata": {},
   "source": [
    "strong associations exist between journeys associated with work and slight casualty severity. However, the strongest association with support of 0.47 and confidence of 0.85 is between journeys with no known purpose and slight casualty severity. \n",
    "\n",
    "This implies that the purpose of journey had little bearing on the severity of accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae676b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize relationship\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "by_purpose.plot(kind='barh', ax=ax, stacked=True, cmap='PuBu')\n",
    "ax.set_title('\\nAccident Severity by purpose of journey\\n', fontsize=14, fontweight='bold')\n",
    "ax.set(xlabel='frequency', ylabel='')\n",
    "ax.legend(bbox_to_anchor=(1.25, 0.98), frameon=False)\n",
    "\n",
    "sns.despine(top=True, right=True, left=True, bottom=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to save plots as png files\n",
    "def save_plot(file_paths, figures): #accepts list of file paths and a list of figures\n",
    "    figs_to_save = {} #create an empty dictionary\n",
    "    for i,j in zip(file_paths, figures):\n",
    "        figs_to_save[i]= j #assign file path as key and figure as value and populate figs_to_save\n",
    "    for file_path, figure in figs_to_save.items(): #for items in figs_to_save\n",
    "        try:\n",
    "            figure.savefig(file_path) #save figure to file path\n",
    "        except AttributeError: #process attribute error if figure is not a FacetGrid object\n",
    "            figure.get_figure().savefig(file_path) #attempt saving again\n",
    "            \n",
    "    return 'The plots have been saved' #confirm completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = figs \n",
    "paths= []\n",
    "for i in range(len(figs)):\n",
    "    paths.append(f\"Figures/{i}.png\")\n",
    "    \n",
    "save_plot(paths, plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb225f86",
   "metadata": {},
   "source": [
    "# I. Predicting Accidents\n",
    "\n",
    "### Can we make predictions about when and where accidents will occur, and the severity of the injuries sustained from the data supplied to improve road safety? How well do our models compare to government models? \n",
    "\n",
    "We attempt to develop models that will accurately predict the following:\n",
    "\n",
    "1. Time of Accidents\n",
    "\n",
    "2. Location of accidents\n",
    "\n",
    "3. Severity of Accidents.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "i. create dataframes of relevant features and preprocess\n",
    "\n",
    "ii. perform feature engineering to select relevant features\n",
    "\n",
    "iii. build relevant models according to task (regression/classification)\n",
    "\n",
    "iv. fit models\n",
    "\n",
    "v. make predictions\n",
    "\n",
    "vi. Evaluate models (**Metrics are RMSE, r2 score, Accuracy & precision score to take into consideration data imbalance**)\n",
    "\n",
    "vii. Compare results with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Defining useful functions ###################################################\n",
    "\n",
    "def encode_data(df, cols, module, encoder = None) :\n",
    "    \"\"\"\n",
    "        Function to Encode Categorical Features, various methods for encoding are included in this function, to choose a\n",
    "        method,\n",
    "        Parameters :\n",
    "        df = Dataframe\n",
    "        cols = column or columns needed\n",
    "        package = method of packages to use which can be : le, ce_oe,oh,du,fe,mt\n",
    "        we provide the following short form of methods with its meaning as follows :\n",
    "        le = Label Encoding\n",
    "        ce_oe = categorical encoders Ordinal Encoding\n",
    "        oh = One hot encoding\n",
    "        du = dummy encoding\n",
    "        fe = frequency encoding\n",
    "        mt = mean target encoding\n",
    "\n",
    "        encoder = external method needed for encoding- default set to none\n",
    "    \"\"\"\n",
    "    encoded_df = df[cols] #Define dataframe to be encoded\n",
    "    if module == \"le\" : # Check for the condition of label encoding\n",
    "        encoded_df = encoded_df.apply(encoder.fit_transform) # Encode the dataframe\n",
    "        return encoded_df # return the encoded feature\n",
    "    \n",
    "    elif module == \"ce_oe\" : # Check for categorical encoding condition\n",
    "        encoded_df = encoder.fit_transform(encoded_df) # Fit and transform the data using categorical encoding\n",
    "        return encoded_df # Return encoded feature\n",
    "    \n",
    "    elif module == \"oh\" : # Check for one hot encoding condition\n",
    "        encoded_df = pd.DataFrame(encoder.fit_transform(encoded_df).toarray()) # fit and transform features using one hot encoding\n",
    "        return encoded_df # Return the encoded data\n",
    "    \n",
    "    elif module == \"du\" : # Check condition for get dummy \n",
    "        encoded_df = pd.get_dummies(df[cols]) # Dummy the features\n",
    "        return encoded_df # Return encoded features\n",
    "    \n",
    "    elif module == 'fe' : # Check for the frequency encoder\n",
    "        encoded_df = df[cols] # Create a dataframe for the encoder\n",
    "        y = encoded_df.groupby(cols).size().reset_index() # Groupby the frequency of the values of the features to encode\n",
    "        y.columns = [cols[0], 'freq'+cols[0]] # Change the column names to the groupby names\n",
    "        encoded_df = pd.merge(encoded_df,y,on = cols[0], how = 'left') # Merge with dataframe\n",
    "        return encoded_df # return encoded features\n",
    "    \n",
    "    elif module == \"mt\" : #Check for mean target encoding\n",
    "        encoded_df = df[cols] # Create its dataframe\n",
    "        x = encoded_df.groupby([cols[0]])[cols[1]].sum().reset_index() # Get the feature sum by grouping by the feature and target then getting the sum.\n",
    "        x = x.rename(columns={cols[1]:cols[0]+\"_\"+cols[1]+\"_sum\"})# rename the columns to the grouped columns names\n",
    "\n",
    "        y = encoded_df.groupby([cols[0]])[cols[1]].count().reset_index()# get feature counts by groupby the column to encode and the target\n",
    "        y = y.rename(columns={cols[1] :cols[0]+\"_\"+cols[1]+\"_Count\"}) # Rename the count columns\n",
    "\n",
    "        z = pd.merge(x,y,on = cols[0], how = 'inner') # Merge the columns together\n",
    "\n",
    "        z['Target_enc_levels'] = z[cols[0]+\"_\"+cols[1]+\"_sum\"]/z[cols[0]+\"_\"+cols[1]+\"_Count\"] # Create a new feature by geting the mean \n",
    "        encoded_df = pd.merge(encoded_df,z,on = cols[0], how = 'left') # Merge it with the dataframe\n",
    "\n",
    "        return encoded_df # Return the encoded feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7997b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaler(df, cols, method):\n",
    "    \"\"\"\n",
    "        Function to scale a dataframe using various of methods.\n",
    "        This function accepts a dataframe, \n",
    "        columns to scale and,\n",
    "        the\n",
    "        methods for scaling them, \n",
    "\n",
    "        It processes and returns a dataframe of scaled values\n",
    "        The methods are :\n",
    "        mmx == Minmax Scaler\n",
    "        sts == Standard Scaler\n",
    "        rbs == Robust Scaler\n",
    "    \"\"\"\n",
    "    scaled_df = df[cols] # Create a dataframe to scale\n",
    "     #for minmax scaler\n",
    "    if method.lower() == \"mmx\": # Check for minmax scaling condition\n",
    "        scaler = MinMaxScaler() # Define the scaler\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(scaled_df)) # Fit and transform data with the scaler\n",
    "        scaled_df.columns = cols # Assign column names to the scaled columns\n",
    "        return scaled_df # Return scaled data\n",
    "    \n",
    "    #for standard scaler\n",
    "    elif method.lower() == \"sts\": # Check for standard scaler condition\n",
    "        scaler = StandardScaler() # Define the scaler\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(scaled_df)) # Fit and transform the data using the scaler\n",
    "        scaled_df.columns = cols # Assign column names to the scaled data\n",
    "        return scaled_df# Return the dataframe\n",
    "    \n",
    "    #for robust scaler\n",
    "    elif method.lower() == \"rbs\": # Check condition for robust scaler\n",
    "        scaler = RobustScaler() # Define the scaler\n",
    "        scaled_df = pd.DataFrame(scaler.fit_transform(scaled_df))# fit and transform the data using the scaler\n",
    "        scaled_df.columns = cols # Assign column names to the dataframe\n",
    "        return scaled_df # Return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrincipalCA(data, n_components, columns):\n",
    "    \"\"\"\n",
    "        Function to perform the Principal Component Analysis on the dataframe.\n",
    "        A feature engineering technique that uses distance\n",
    "        to get component from data, and takes care of overlapping features.\n",
    "        Parameters are, a dataframe, amount of component \n",
    "        columns to be derived and the initial columns names\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components) # Initializing PCA\n",
    "    principalComponents = pca.fit_transform(data) # Fit and transform data using the initialised PCA\n",
    "    principalDf = pd.DataFrame(data= principalComponents,columns = columns) # Convert the transformed data to a dataset\n",
    "    return principalDf # Return transformed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, random_state, test_size):\n",
    "    \"\"\"\n",
    "        This is a function to split the dataframe into train and test data.\n",
    "        parameters are features, label, random_state, test_size, and variables to store split data.\n",
    "    \"\"\"\n",
    "    Xtrain, Xtest, y_train, y_test = train_test_split(X,y,random_state=random_state,test_size=test_size) # Split data and assign to variables\n",
    "    print(Xtrain.shape) # Print shape for train features\n",
    "    print(Xtest.shape) # Print shape for test features\n",
    "    print(y_train.shape)# Print shape for train labels\n",
    "    print(y_test.shape) # Print shape for test labels\n",
    "    return Xtrain, Xtest, y_train, y_test # Return variables for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8346696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressors(train_features, train_label, test_features, test_label, models):\n",
    "    \"\"\"\n",
    "    Function builds multiple regression models using different algorithms with default parameters \n",
    "    and returns their metrics.\n",
    "    it accepts train_features, train_label, test_features, test_label, and the list of models\n",
    "    \"\"\"\n",
    "    scores=pd.DataFrame(columns=['Model', 'RMSE', 'r2_score'])\n",
    "   # predictions=[]\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(train_features,train_label) # Fit model\n",
    "        y_pred = model.predict(test_features) # predict test labels using the test features\n",
    "        model_name = type(model).__name__ #get model name\n",
    "       \n",
    "        \n",
    "        #predictions.append(y_pred)\n",
    "        \n",
    "         # Evaluate model with RMSE and R2 score\n",
    "        scores= scores.append({'Model':model_name,\n",
    "                               'RMSE':np.sqrt(mean_squared_error(test_label,y_pred)),\n",
    "                               'r2_score': r2_score(test_label,y_pred)}, ignore_index=True)\n",
    "        \n",
    "    #create dataframe of models and predictions    \n",
    "    #pred_df = pd.DataFrame(predictions.transpose(), columns = ['LR', 'KN', 'RF', 'GB'])\n",
    "\n",
    "\n",
    "    \n",
    "    return scores#, pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifiers(train_features, train_label, test_features, test_label, models):\n",
    "    \"\"\"\n",
    "    Function builds multiple classifier models using different algorithms with default parameters \n",
    "    and returns their metrics.\n",
    "    it accepts train_features, train_label, test_features, test_label, and the list of models\n",
    "    \"\"\"\n",
    "    scores=pd.DataFrame(columns=['Model', 'Accuracy', 'Precision'])\n",
    "    #predictions= []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(train_features,train_label) # Fit model\n",
    "        y_pred = model.predict(test_features) # predict test labels using the test features\n",
    "        model_name = type(model).__name__ #get model name\n",
    "       \n",
    "        \n",
    "        #predictions.append(y_pred)\n",
    "        \n",
    "         # Evaluate model with RMSE and R2 score\n",
    "        scores= scores.append({'Model':model_name,\n",
    "                               'Accuracy':accuracy_score(test_label,y_pred),\n",
    "                               'Precision': precision_score(test_label,y_pred, average='weighted')}, ignore_index=True)\n",
    "        \n",
    "    #create dataframe of models and predictions    \n",
    "    #pred_df = pd.DataFrame (predictions.transpose(), columns = ['LR', 'KN', 'RF', 'GB'])\n",
    "\n",
    "\n",
    "    \n",
    "    return scores#, pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504eaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking(task, X:list, Y:list, n_split):\n",
    "    if task.lower() == 'regression':\n",
    "        \n",
    "        # get a stacking ensemble of base models\n",
    "        level0 = list()\n",
    "        level0.append(('lr', LinearRegression()))\n",
    "        level0.append(('knn', KNeighborsRegressor()))\n",
    "        level0.append(('rfc', RandomForestRegressor()))\n",
    "        level0.append(('GBr', GradientBoostingRegressor()))\n",
    "        \n",
    "        # define meta learner model\n",
    "        level1 = LinearRegression()        \n",
    "\n",
    "        #simple kfold since target may be continuous\n",
    "        kfold = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "        \n",
    "        # define the stacking ensemble\n",
    "        stack_model = StackingRegressor(estimators=level0,\n",
    "                                        final_estimator=level1, passthrough=False,\n",
    "                                        cv=kfold)\n",
    "\n",
    "        stack_model.fit(X[0], Y[0])\n",
    "        R_pred = stack_model.predict(X[1])\n",
    "        \n",
    "        rmse =np.sqrt(mean_squared_error(Y[1], R_pred))\n",
    "        r2 =r2_score(Y[1],R_pred)\n",
    "        \n",
    "        print(f' The RMSE of the stack model is {rmse}')\n",
    "        print(f' The r2_score of the stack model is {r2}')\n",
    "        return stack_model\n",
    "    \n",
    "    elif task.lower() == 'classification':\n",
    "\n",
    "        # get a stacking ensemble of base models\n",
    "        level0 = list()\n",
    "        level0.append(('xgb', XGBClassifier()))\n",
    "        level0.append(('knn', KNeighborsClassifier()))\n",
    "        level0.append(('rfc', RandomForestClassifier()))\n",
    "        level0.append(('gbc', GradientBoostingClassifier()))\n",
    "        \n",
    "        # define meta learner model\n",
    "        level1 = XGBClassifier()        \n",
    "\n",
    "        \n",
    "        kfold = KFold(n_splits=n_split, shuffle=True, random_state=42)\n",
    "\n",
    "        stack_model = StackingClassifier(estimators=level0,\n",
    "                                        final_estimator=level1, passthrough=False,\n",
    "                                        cv=kfold)\n",
    "\n",
    "        stack_model.fit(X[0], Y[0])\n",
    "        C_pred = stack_model.predict(X[1])\n",
    "        \n",
    "        acc= accuracy_score(Y[1], C_pred)\n",
    "        prec=precision_score(Y[1], C_pred, average = 'weighted')\n",
    "\n",
    "        print(f' The Accuracy of the stack model is {acc}')\n",
    "        print(f' The Precision of the stack model is {prec}')\n",
    "        return stack_model\n",
    "    \n",
    "    else:\n",
    "        print(f'please define an appropriate task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " def Cross_val(X, y, n_splits, n_repeats, model_list, scoring):\n",
    "    \"\"\"\n",
    "    Function to compare various models using cross validation\n",
    "    \"\"\"\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats= n_repeats, random_state=1)\n",
    "    model_scores=[]\n",
    "    \n",
    "    for model in model_list:\n",
    "        score = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1, error_score='raise')\n",
    "        score = np.mean(scores)\n",
    "        model_scores.append(score)\n",
    "    return model_scores\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating original dataframe\n",
    "\n",
    "#To reconcile Accidents and Analysis together\n",
    "df = pd.merge(Accidents,Analysis, on = \"Accident_Index\", how = \"left\",  suffixes=('', '_drop')) \n",
    "\n",
    "# drop duplicated rows\n",
    "df.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "# drop duplicated columns\n",
    "df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "\n",
    "#fill in missing values\n",
    "fill_missing(df) #For forward fill\n",
    "fill_missing(df, 'backfill') #For backward fill\n",
    "\n",
    "df.info() #Check df especially for missing values\n",
    "#no need to forcefully drop vehicle reference as it is not relevant for analysis and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2785431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model lists\n",
    "Regressors = [LinearRegression(), KNeighborsRegressor(), RandomForestRegressor(), GradientBoostingRegressor()]\n",
    "\n",
    "Classifiers = [XGBClassifier(), KNeighborsClassifier(), RandomForestClassifier(), GradientBoostingClassifier()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75792e0",
   "metadata": {},
   "source": [
    "### Predicting time of accidents\n",
    ">Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa951f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset based on intuitive feature importance\n",
    "Time = df[['Weather_Conditions','week','Hour','Day_of_Week','Light_Conditions','Special_Conditions_at_Site',\n",
    "           'decimal_time', 'Location_Easting_OSGR','Location_Northing_OSGR','LA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ca7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables\n",
    "Time_cat = ['Weather_Conditions','Light_Conditions','Special_Conditions_at_Site','LA']\n",
    "\n",
    "# encode categotical data\n",
    "Time[Time_cat] = encode_data(Time, Time_cat, \"le\",LabelEncoder())\n",
    "Time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_X = Time.drop('decimal_time',axis=1)\n",
    "time_y = Time['decimal_time']\n",
    "time_X_scaled = Scaler(time_X, time_X.columns, 'rbs')\n",
    "pca_time = PrincipalCA(time_X_scaled, 4, ['col1','col2','col3','col4'])\n",
    "pca_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "time_X_train,time_X_test,time_y_train,time_y_test = split_data(pca_time, time_y, 42, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f71663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit regression models\n",
    "scores = regressors(time_X_train,time_y_train,time_X_test,time_y_test, Regressors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb62168",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dda5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack models\n",
    "time_stack = Stacking('regression',[time_X_train,time_X_test],[time_y_train,time_y_test],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append scores to model df\n",
    "scores= scores.append({'Model':'stacked_model',\n",
    "                        'RMSE':0.7115149,\n",
    "                       'r2_score':0.9808938}, ignore_index=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced37e1",
   "metadata": {},
   "source": [
    "### Predicting location of accidents\n",
    ">Classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset based on intuitive feature importance\n",
    "Location_data = df[['Location_Easting_OSGR','Location_Northing_OSGR','Road_Surface_Conditions','Road_Type',\n",
    "                   'Urban_or_Rural_Area','loc_cluster', \"dist_from_null\", 'Day_of_Week', 'LA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset\n",
    "Location_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b719117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables\n",
    "Location_cat = ['Location_Easting_OSGR','Location_Northing_OSGR','Road_Surface_Conditions','Road_Type',\n",
    "                   'Urban_or_Rural_Area', 'Day_of_Week', 'LA']\n",
    "\n",
    "#Encode Categorical Features\n",
    "Location_data[Location_cat] = encode_data(Location_data,Location_cat, \"le\",LabelEncoder())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64866be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_X = Location_data.drop('loc_cluster',axis=1) #features\n",
    "loc_y = Location_data['loc_cluster'] #label\n",
    "\n",
    "#scale data to normalise\n",
    "loc_Xscaled = Scaler(loc_X, loc_X.columns, 'rbs')\n",
    "\n",
    "# Engineer Features using PCA\n",
    "pca_loc = PrincipalCA(loc_Xscaled, 4, ['col1','col2','col3','col4'])\n",
    "pca_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using train test split\n",
    "loc_X_train,loc_X_test,loc_y_train,loc_y_test = split_data(pca_loc, loc_y, 42, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate classification models\n",
    "scores2= classifiers(loc_X_train,loc_y_train,loc_X_test,loc_y_test, Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21983cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack models\n",
    "loc_stack1 = Stacking('classification',[loc_X_train,loc_X_test],[loc_y_train,loc_y_test,],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append scores to model df\n",
    "scores2= scores2.append({'Model':'stacked_model',\n",
    "                               'Accuracy':0.999194,\n",
    "                               'Precision':0.999194}, ignore_index=True)\n",
    "scores2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f59b",
   "metadata": {},
   "source": [
    ">Regression task\n",
    "\n",
    "Predicting distance from null using coordinates in Euclidean distance is ideal since the data is local to a small area(The UK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1458a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features using PCA\n",
    "location_X = Location_data.drop('dist_from_null',axis=1)\n",
    "location_y = Location_data ['dist_from_null']\n",
    "\n",
    "# Scale data\n",
    "location_data = Scaler(location_X, location_X.columns, 'rbs')\n",
    "pca_loc2 = PrincipalCA(location_data, 4, ['col1','col2','col3', 'col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using train_test_split\n",
    "loc2_X_train,loc2_X_test,loc2_y_train,loc2_y_test = split_data(pca_loc2, location_y, 42, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bb139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit regression models\n",
    "scores3 = regressors(loc2_X_train,loc2_y_train,loc2_X_test,loc2_y_test, Regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba120930",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack models\n",
    "loc_stack2=Stacking('regressors',[loc2_X_train,loc2_X_test],[loc2_y_train,loc2_y_test],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a669d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append scores to model df\n",
    "scores3= scores3.append({'Model':'stacked_model',\n",
    "                        'RMSE':4.178288,\n",
    "                         'r2':0.999734, ignore_index=True})\n",
    "scores3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f56a3",
   "metadata": {},
   "source": [
    "### Predicting severity of accidents\n",
    ">Clasification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset based on intuitive feature importance\n",
    "\n",
    "Severity = df[['Speed_limit','Weather_Conditions','Age_of_Casualty','Age_of_Driver','Casualty_Class'\n",
    "              ,'Propulsion_Code','Casualty_Severity','Pedestrian_Movement','Light_Conditions',\n",
    "               'Pedestrian_Location','Day_of_Week','Hour','LA','Carriageway_Hazards']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables\n",
    "Severity_cat = ['Weather_Conditions','Propulsion_Code','Casualty_Class','Propulsion_Code',\n",
    "                'Carriageway_Hazards','Light_Conditions',\n",
    "                'Casualty_Severity','Pedestrian_Movement','Pedestrian_Location','LA']\n",
    "\n",
    "#Encode Categorical Features\n",
    "Severity[Severity_cat] = encode_data(Severity,Severity_cat, \"le\",LabelEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_X = Severity.drop('Casualty_Severity',axis=1) # Features\n",
    "sev_y = Severity['Casualty_Severity'] # Label\n",
    "\n",
    "#scale data to normalise\n",
    "sev_X_scaled = Scaler(sev_X, sev_X.columns, 'rbs')\n",
    "# Engineer Features using PCA\n",
    "pca_sev = PrincipalCA(sev_X_scaled, 4, ['col1','col2','col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using train test split\n",
    "sev_X_train,sev_X_test,sev_y_train,sev_y_test = split_data(pca_sev, sev_y, 42, 0.25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64605f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate classification models\n",
    "scores4 = classifiers(sev_X_train,sev_y_train,sev_X_test,sev_y_test, Classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack models\n",
    "sev_stack=Stacking('classification',[sev_X_train,sev_X_test],[sev_y_train,sev_y_test,],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append scores to model df\n",
    "scores4= scores4.append({'Model':'stacked model',\n",
    "                               'Accuracy': 0.849396,\n",
    "                               'Precision':0.804295}, ignore_index=True)\n",
    "scores4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea62c7",
   "metadata": {},
   "source": [
    "### Baseline comparison- Government Predictions\n",
    "\n",
    ">Government models were used to predict for only the slight or serious severity class, Manipulation of the data is required in order to compare the predictions with our model's.  In addition, while, we are only extrapolating for 2019, the governments predictions run for several years with millions of values. We will also only deal with relevant entries from the government adjustment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident severity data\n",
    "cas = df[['Accident_Index','Speed_limit','Weather_Conditions','Age_of_Casualty','Age_of_Driver','Casualty_Class',\n",
    "              'Propulsion_Code','Casualty_Severity','Pedestrian_Movement',\n",
    "               'Pedestrian_Location','Day_of_Week','Hour','LA','Carriageway_Hazards']]\n",
    "\n",
    "# get df of govt. adjustment file\n",
    "cas_adj = pd.read_csv('cas_adjustment_lookup_2019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8513b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cas), len(cas_adj))\n",
    "\n",
    "''' length of both dataframes do not match to make a comparison of values. The difference is too large to merge both \n",
    "further analysis will therefore be carried out on just casualties and accidents data eliminating all \n",
    "vehicle features'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To merge Accidents and casualty together\n",
    "cas_acc = pd.merge(Accidents,casualties, on = \"Accident_Index\", how = \"left\",  suffixes=('', '_drop')) \n",
    "\n",
    "# drop duplicated rows\n",
    "cas_acc.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "# drop duplicated columns\n",
    "cas_acc.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)\n",
    "\n",
    "#fill in missing values\n",
    "fill_missing(cas_acc) #For forward fill\n",
    "fill_missing(cas_acc, 'backfill') #For backward fill\n",
    "\n",
    "cas_acc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = cas_acc[['Accident_Index','Speed_limit','Weather_Conditions','Age_of_Casualty','Casualty_Class',\n",
    "              'Casualty_Severity','Pedestrian_Movement','Light_Conditions','Casualty_Type',\n",
    "               'Pedestrian_Location','Day_of_Week','Hour','LA','Carriageway_Hazards']]\n",
    "\n",
    "# drop the fatal class\n",
    "cas = cas[(cas['Casualty_Severity'] == 2) | (cas['Casualty_Severity'] == 3)]\n",
    "\n",
    "# match accident index column for both files\n",
    "cas_adj.rename(columns={'accident_index': 'Accident_Index'}, inplace=True)\n",
    "\n",
    "# get index for 2019 entries\n",
    "sev_indx = cas_acc['Accident_Index'].unique()\n",
    "\n",
    "#get 2019 from adjustments\n",
    "cas_adj = cas_adj[cas_adj['Accident_Index'].isin(sev_indx)]\n",
    "\n",
    "cas_adj['Accident_Index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb53bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both dfs to match lengths(important for evaluation)\n",
    "cas_sev_adj = pd.merge(cas, cas_adj)\n",
    "\n",
    "cas_sev_adj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3e2e7",
   "metadata": {},
   "source": [
    "We get evaluate the adjusted predictions against the actual values to get the baseline scores for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert adj predictions to binary classes and generate column\n",
    "cas_sev_adj['sev'] = [ 1 if (i>=0.5 and j<0.5) else 0 for i,j in zip(cas_sev_adj['Adjusted_Serious'], cas_sev_adj['Adjusted_Slight'])]\n",
    "cas_sev_adj['sev'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cas_sev_adj['Casualty_Severity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32caf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual classes to match the now binary adjusted predictions, serious accidents are the positive class 1\n",
    "cas_sev_adj['Cas_Sev'] = [ 1 if x== 2 else 0 for x in cas_sev_adj['Casualty_Severity']]\n",
    "cas_sev_adj['Cas_Sev'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=accuracy_score(cas_sev_adj['Cas_Sev'], cas_sev_adj['sev'])\n",
    "prec=precision_score(cas_sev_adj['Cas_Sev'], cas_sev_adj['sev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate government models using accuracy score and precision\n",
    "print(f'government model predicted accidents severity for 2019 with an accuracy of {acc}')\n",
    "print(f'government model predicted accidents severity for 2019 with an precision of {prec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predicting with the best existing model for the scenario'''\n",
    "# data\n",
    "Sev_data = cas_sev_adj[['Speed_limit','Weather_Conditions','Age_of_Casualty','Casualty_Class',\n",
    "              'Casualty_Severity','Pedestrian_Movement','Light_Conditions','Casualty_Type',\n",
    "               'Pedestrian_Location','Day_of_Week','Hour','LA','Carriageway_Hazards','Cas_Sev']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical variables\n",
    "Sev_data_cat = ['Speed_limit','Weather_Conditions','Casualty_Class',\n",
    "                'Casualty_Type','Pedestrian_Movement','Light_Conditions',\n",
    "               'Pedestrian_Location','Speed_limit','Cas_Sev','LA']\n",
    "\n",
    "#Encode Categorical Features\n",
    "Sev_data[Sev_data_cat] = encode_data(Sev_data,Sev_data_cat, \"le\",LabelEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev_data_X = Sev_data.drop('Cas_Sev',axis=1) # Features\n",
    "sev_data_y = Sev_data['Cas_Sev'] # Label\n",
    "\n",
    "#scale data to normalise\n",
    "sev_data_X_scaled = Scaler(sev_data_X, sev_data_X.columns, 'rbs')\n",
    "\n",
    "# Engineer Features using PCA, set n_components to 4 to capture significant variance\n",
    "pca_sev_data = PrincipalCA(sev_data_X_scaled, 4, ['col1','col2','col3','col4'])\n",
    "\n",
    "#split data without further feature engineering\n",
    "sev_data_X_train,sev_data_X_test,sev_data_y_train,sev_data_y_test = split_data(pca_sev_data, sev_data_y, 42, 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54614406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model (using current best model)\n",
    "model = sev_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit  model\n",
    "model.fit(sev_data_X_train,sev_data_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92935577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "stack_pred = model.predict(sev_data_X_test)\n",
    "\n",
    "# evaluate\n",
    "model_acc = accuracy_score(sev_data_y_test, stack_pred)\n",
    "model_prec = precision_score(sev_data_y_test, stack_pred)\n",
    "\n",
    "print(f'The stacking model predicted accidents severity for 2019 with an accuracy of {model_acc}')\n",
    "print(f'The stacking model predicted accidents severity for 2019 with a precision of {model_prec}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc42526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
